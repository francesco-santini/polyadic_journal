\documentclass{llncs}
\usepackage{mathptmx}

\usepackage{amsmath,amssymb,amsxtra,amsfonts,cancel}
\usepackage{graphicx,paralist}
\usepackage{url}
\usepackage{tikz-cd}
\usetikzlibrary{trees, arrows}
\usepackage{xspace}
%\usepackage{hyperref}
\usepackage{setspace}
\usepackage{tikz}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{textcomp}
\usepackage{soul}
\usepackage{listings}
\usepackage{mathtools}

\usepackage{todonotes}
% To disable notes without deleting them
%\usepackage[disable]{todonotes}

%\floatstyle{plain}
%\newfloat{myalgo}{tbhp}{mya}

\newenvironment{Algorithm}[2][tbh]%
{\begin{myalgo}[#1]
		\centering
		\begin{minipage}{#2}
			\begin{algorithm}[H]}%
			{\end{algorithm}
		\end{minipage}
	\end{myalgo}}
% to cut ------------------------------------------------------
%\usepackage{paralist}
%\usepackage[small]{caption}
%\usepackage{textcomp}
%\usepackage{times}
%\addtolength{\floatsep}{-5mm} \addtolength{\textfloatsep}{-5mm}
% -------------------------------------------------------------

\newtheorem{define}[theorem]{Definition}

\newtheorem{exa}[theorem]{Example}
\def\smallromani{\renewcommand{\theenumi}{\roman{enumi}}
        \renewcommand{\labelenumi}{(\theenumi)}}

%\def\bigodiv{{ \mathbf{\bigodot \hspace{-11pt} \boxempty \,\,}}}

\def\bigodiv{ {\text{ \large $\mathbf\odiv\hspace{-9.3pt} \div$}} }
\def\bigominus{ {\text{ \large $\mathbf\odiv\hspace{-9.3pt} -$}} }


%\defodiv{{ \odiv\hspace{-7.5pt} \div}}
\def\0{{\mathbf 0}}
\def\1{{\mathbf 1}}
\def\C{{\mathcal C}}
\newcommand{\rrarrow}{\longrightarrow}
\newcommand{\diag}[2]{d_{{#1}{#2}}}
\newcommand{\comment}[1]{}
\newcommand{\tell}{{\bf tell}}
\newcommand{\atell}{{\bf atell}}
\newcommand{\ask}{{\bf ask}}
\newcommand{\ostop}{{\bf stop}}
\newcommand{\retract}{{\bf retr}}
\newcommand{\rarrow}{\rightarrow}
\newcommand{\remove}{\rightarrow}
%introdotto per rimuovere le prove
\newcommand{\shortNoProof}[1]{ }

\def\ent{\vdash}
\def\monid{{\mathbf 0}}
\def\1{{\mathbf 1}}
\def\C{{\mathcal C}}
\def\K{{\mathcal K}}
\long\def\comment#1{}
\def\monop{\otimes}
\def\odiv{\, {\ominus\hspace{-7.7pt} \div} \,}
\def\monid{\mathbf{1}}

\newcommand{\SCCP}{\texttt{SCCP}\xspace}
\newcommand{\RefFig}[1]{Figure \nolinebreak\ref{#1}}
\newcommand\fnsep{\textsuperscript{,}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{Polyadic Soft Constraints~\thanks{Research partially supported by the MIUR PRIN 2017FTXR7S ``IT-MaTTerS''.}}


\author{Laura Bussi\inst{1}, Fabio Gadducci\inst{1}, 
Francesco Santini\inst{2}
} 
	\institute{Dipartimento di Informatica, University of Pisa, Italy \\
		\email{laura.bussi@phd.unipi.it} \qquad
		\email{fabio.gadducci@unipi.it}
		\and Dipartimento di Matematica e Informatica, University of Perugia, Italy\\
		\email{francesco.santini@unipg.it}
		}
	
\titlerunning{Polyadic Soft Constraints}
\authorrunning{Bussi, Gadducci, and Santini}

\maketitle

\begin{abstract}
We propose a formalism for manipulating soft constraints based on polyadic algebras.
%
The choice of such algebras in place of classical cylindric ones
 simplifies the structure of the partial order of preference values by removing diagonals, 
 a family of constants used for modelling parameter passing and variable substitution,
 whose presence require completeness. 
 Removing diagonals also allows for an easy representation of preference/cost functions in terms of polynomials,
 thus streamlining their manipulation on languages based on (stores of) constraints.
%
Besides presenting the main features of the new formalism,
the paper investigates how the operators of polyadic algebras interact with the residuated 
monoid structure tat is used for representing the set of preference values. 

%
%The constraint store that describes a \emph{Soft Constraint Satisfaction Problem} (\emph{SCSP}) or the result of agents interaction in \emph{Soft Concurrent Constraint Programming} (\emph{SCCP}), 
%s then represented by a polynomial, as all constraints in general, 
%Our main objective is to provide a blueprint for  developing  such frameworks in practice, by filling the gap between the original algebraic approach and an actual implementation.
\end{abstract}

\keywords{Soft constraints, polyadic algebras, residuated monoids.}

\section{Introduction}\label{sec:intro}

%\subsection{Constraints and constraint programming}
Computer scientists often face combinatorial problems, as e.g. in operational research, artificial intelligence, or circuit design,
and constraints are a tool for naturally modelling such problems. % in a natural way. 
In their simplest form, constraints are just sets of inequations: % over a domain: 
given a set of variables $V$ and a domain of values $D$, 
a constraint over a subset of variables limits the combinations of values that such variables can take. 
Combinatorial problems can thus be easily modelled by constraints: \emph{constraint satisfaction problems} (CSPs)
are defined over a set $V$, a domain $D$ and a set of constraints $C$. Solving a CSP
simply means to find an assignment of the variables such that all its constraints are satisfied. 

\begin{samepage}
Consider, for instance, a problem where we want to respect a given relation among two measures, e.g. $y = 2x$, for $x$ representing the width and $y$ the length of an object. 
Of course, both $x$ and $y$ have to be positive reals and might have to respect some upper bounds, let us say $5$ and $9$. The problem can be formalised as
\begin{itemize}
	\item the set of variables $V$ is $\{x,y\}$;
	\item the domain $D$ is the set of positive reals $\mathbb{R}^+$;
	\item the constraints in $C$ are given by polynomials: $\{2x-y = 0, \ x-5 \leq 0, \ y-9 \leq 0\}$.
\end{itemize}
\noindent
where clearly the assignment $\{x \leftarrow 4, \ y \leftarrow 8\}$ is a solution for the problem.
\end{samepage}

%\smallskip
%\newpage
\emph{Constraint programming} (CP) is a computational framework that allows to implement algorithms for solving CSPs:  
it has been intensively studied and a set of algorithms for solving CSPs have been implemented since the Nineties~\cite{aijour}. 
%
\emph{Concurrent constraint programming} (CCP) extends CP by allowing parallel programs to interact by means of a shared store representing a constraint. 
%
As the basic operations in imperative programming are \emph{read} and \emph{write}, in CP and CCP we have operations to manipulate the store
\begin{itemize}
	\item \emph{tell(c)} adds a constraint \emph{c} to the store;
	\item \emph{ask(c)} checks if a constraint \emph{c} is satisfied, i.e. the store \emph{entails} such a constraint;
\end{itemize}
\noindent
and depending on the domain we may additionally require subtraction (as for example it has been implemented in \cite{lcc}, 
where constraints are formulas in linear logic)
\begin{itemize}
	\item \emph{remove(c)} removes \emph{c} from the store, thus possibly lifting some of the requirements that had been enforced by the constraint. 
	%After the removal, values which were allowed by \emph{c} could be permitted, according on the remaining constraints are satisfied. 
\end{itemize}
%
%Constraint programming have been intensively studied and a set of algorithms for solving CSPs have been implemented since the early Nineties.~\cite{aijour}. 
%Furthermore, CP allows for concurrency by using a blocking \emph{ask} operation: this led to the definition of concurrent constraint programming, 
%where multiple agents can access the store at the same time and perform parallel operations~\cite{popl91}. 
%
In order to model e.g. procedure calls, furthers operators are needed on constraints. 
%
The standard solution~\cite{popl91} is via cylindric algebras: a family of variable-indexed unary operations, %called \emph{cylindrification}, 
for representing existential quantifiers, and a family of constants called \emph{diagonals}, 
for modelling parameters passing and variable substitution.

\smallskip
Despite CSP expressiveness,  
in many real-life situations we need a more flexible way to model problems, 
since some constraints could be ``less important'' than others and might not have to be necessarily satisfied.
%
Soft constraints have been introduced to model this kind of situation. In informal terms, they are classical constraints where a value from a partially ordered set is associated to each instantiation of the variables of a constraint. It is  thus possible to state that a constraint is either  more or less significant than others in order to find a good approximation of the solution, even if not all constraints are satisfied at the same time. 
%
Then, a \emph{soft constraint} is defined as a function of type $(V\to D)\to A$, 
where functions $V\to D$ are assignments of variables in $V$ to a domain $D$ and the values in $A$ represent either cost or preferability, depending on whether representing negative or positive preferences. Combining positive and negative preferences results in what is called a bipolar approach~\cite{posneg}.

To evaluate preferences, the set $A$ is equipped with a semi-lattice structure and, 
to make possible the combination of soft constraints, a monoidal operator. 
Therefore, $A$ turns out to be an idempotent semiring (also called tropical semiring or dioid). 
%
%Many such structures have been proposed.
 For instance, \emph{Fuzzy} semirings associate a constraint to its cost and the aim is to minimise the sum,
  %this falls into the range of negative preferences. 
  while \emph{Probabilistic} semirings associate a constraint to its probability and the aim is to maximise the joint probability.
%Our proposal follows instead a bipolar approach.
A further operation is needed over the set $A$, in order to model the removal of constraints, 
resulting in a residuated monoid: the basics of the formalism within the bipolar approach have been presented in~\cite{ipl17}.

%However, one of the issues that was left open is the representation of constraints in a compact form, since a polynomial presentation is not readily available:
%indeed, tackling this problem has been the starting point of our work.

%\subsection{Cylindric and polyadic algebras for soft constraints}
\medskip

Some extensions of CCP for allowing soft constraints have been already proposed, as for instance in~\cite{scc}, and they required a reworking of the underlying theory.
The starting point has been again the operators of cylindric algebras. However, the use of diagonals for soft constraints puts some strong requirements on the structure of the set of values $A$, 
which are not always met in actual case studies. Moreover, diagonals also present some serious issues in finding a compact representation.

\comment{
Some extensions of CCP for allowing soft constraints have been already proposed, as for instance in~\cite{scc} and they have obviously required a rework of the underlying theory.
in order to provide new operators for modelling variables hiding and procedure calls. These are mainly based on the notion of cylindric algebra, which was defined by Tarsky in 1952 and represents an algebraisation of first order logic. In rough terms, a cylindric algebra is a Boolean algebra provided with unary operations called \emph{cylindrification}, which are a family of variable-indexed operators representing the existential quantifier. Cylindric algebras are also provided with constants, called \emph{diagonals}, which are are used for modelling parameters passing and variables substitution. As said, cylindric algebras have been used in formalisation of constraints, as well as for an algebraisation of relational models~\cite{cylalg}.

Our objective is to develop a more general framework for modelling bipolar preferences in the soft constraint formalism, by providing a new set of operators and defining their properties in order to represent a larger number of interesting case studies.

Polyadic algebras have been introduced by Halmos and they can be considered as a generalisation of cylindric algebras, since they extend the latter and provide a new family of operators which allows for replacing diagonals in modelling variables substitution. Halmos' definition also provides the notion of diagonals and so it's been used so far: indeed, proposed algebras for soft constraints are usually based on complete lattices and structures equipped with diagonals. 

In order to achieve the goal of generalisation, we're completely replacing diagonal elements with a family of weaker operators which recall the given definition of polyadic operators~\cite{sagi2013}. This will allow us to move to semi-lattices and to represent a larger number of case studies. In particular, polynomial functions don't allow for a representation using diagonals: our formalisation fills this gap, providing a comfortable way to express polynomials in the soft constraints formalism.
}%comment

The proposal developed in this paper is to consider polyadic algebras instead of cylindric ones. This means to replace diagonals with a family of polyadic operators (see e.g.~\cite{sagi2013}) that
precisely axiomatise variable substitution, and to investigate how these operators interact with the residuated monoid structure of the set of values $A$.

The benefits are twofold. On the one hand, we relax some of the requirements necessary for $A$: the join semi-lattice on $A$ must be complete for cylindric algebras, while it is not necessary so for polyadic ones. Such relaxation is relevant also for more recent developments on CCP: while in standard denotational models the semantics universe is supposed to have a complete semi-lattice structure, this is not needed with 
the bisimulation semantics introduced in \cite{pippo}.
%
On the other hand, replacing diagonals with polyadic operators allows for a compact -- \emph{polynomial} -- representation of soft constraints.

In Section~\ref{sec:bg} we rephrase some basic definitions for residuated monoids. 
In Section~\ref{newpro} we offer a presentation of polyadic algebras tailored for constraints and we present some results 
concerning the combination of these two structures.
%
These results are used in Section~\ref{sec:softconstraints}, where we provide a novel formalisation of soft constraints in terms of polyadic algebras whose carrier is a residuated monoid. 
%
Finally, in Section~\ref{sec:polynomialsoftconstraints} we introduce a set of constraints that enjoy a polynomials representation 
and we discuss the algebra of linear polynomials with coefficients in $\mathbb{N}$ as a case study.

\comment{
\paragraph{The (soft) constraints of Catuscia.}
The word ``constraint'' occurs in the titles of 40 publications co-authored by Catuscia. Between 1991 and 1997, most of these papers were devoted to the semantics~\cite{boer1} and analysis~\cite{de1997proving} of Constraint Logic Programming and Concurrent Constraint Programming. In 2001, the latter language was extended to Temporal Concurrent Constraint Programming~\cite{nielsen2002temporal,palamidessi2001temporal}.
%, a rather influential work, that led to the Ph.D. thesis of Frank Valencia. 
Since then, Catuscia has exploited constraints in several applications domains like security \cite{DBLP:conf/iclp/LopezPPRV06}, biological systems \cite{DBLP:conf/birthday/ChiarugiFOP15}, and the modelling of knowledge in social networks \cite{DBLP:conf/concur/KnightPPV12}.

Despite her scientific interest in hard constraints, Catuscia seems to prefer a rather soft approach in her personal and professional relationships: although always surrounded by students, collaborators, and visitors, Catuscia is always available for chatting and joking, and for helping out, both in technical and in personal matters. 
%
Also well-known among friends is her soft spot for the wee hours of the morning. %, both for personal and technical matters.
%
%The first author remembers that during his fellowship at \'Ecole Polytechnique, Catuscia helped him to solve many issues, personal, administrative, and technical. 
%
In particular, the first author recalls that, while working on the afore-mentioned bisimulation semantics for CCP~\cite{pippo}, after many weeks trying hard to prove the main theorem with the other collaborators, he spent one afternoon in Catuscia's office looking together for a proof. When he woke up the morning after, in his mail-box there was a message from her: a beautiful and crystal-clear proof of the long awaited result. This message was sent at about 3 am.}

\section{An Introduction to Residuated Monoids}\label{sec:bg}

This section recalls some results on residuated monoids,
which are our chosen algebraic structure for modelling
soft constraints.
%
They are mostly drawn from~\cite{jlamp17}, and they are presented without proofs.
Note however that, to the best of our knowledge, the material from Proposition~\ref{reabs} up
to Example~\ref{nodist2} appears to be original in the literature on costraints.
%
%Section 2.2 presents our personal take on polyadic algebras:
%the standard axiomatisation of e.g.~\cite{sagi2013} has been completely 
%reworked, in order 
%to be adapted to the constraints formalism.
%
%Finally, Section~\ref{cyre} offers some preliminary insights on 
%the laws for polyadic operators in residuated monoids.

\subsection{Preliminaries on Ordered Monoids}\label{sec:lem}

The first step is to define an algebraic structure for modelling preferences,
where it is possible to compare values and combine them.
Our choice falls into the range of \emph{bipolar} approaches, in order to represent both positive and negative preferences: 
we refer to~\cite{ipl17} for a detailed introduction and a comparison with other proposals.
% such as~\cite{xxx}\todo{Manca citazione}.

\begin{definition}[partial order]
	A partial order (PO) is a pair $\langle A, \leq \rangle$ such that
	$A$ is a set and 
	$\leq \,\,\subseteq A \times A$ is a reflexive, transitive, and
	anti-symmetric relation.
	% and $\forall a \in A. \bot\leq a$.
	%
	%A partial order with bottom (POT) is a triple
	%$\langle A, \leq, \bot \rangle$ such that $\langle A, \leq \rangle$ is a PO and
	%$\forall a \in A. \bot \leq a$.
	%
	A (join) semi-lattice (SL) is a PO such that any non-empty finite  subset of $A$ has a
	least upper bound (LUB).
\end{definition}

%We write 
The LUB of a (possibly infinite or empty) subset $X \subseteq A$ is denoted $\bigvee X$, and it is clearly unique.
Should  they exist, $\bigvee A$ and $\bigvee \emptyset$ correspond respectively to the top, denoted as 
$\top$, and to the bottom, denoted as $\bot$, of the PO.

\comment{\begin{definition}[Compact elements]
An element $a \in A$ is compact 
%(or finite) 
if whenever $a \leq \bigvee Y$ for some $Y \subseteq A$
there exists a finite subset
$X \subseteq Y$ such that $a \leq \bigvee X$.
%
%Let $A^C \subseteq A$ be the set of compact elements of ${\mathbb C}$.
%Then ${\mathbb C}$ is algebraic if $\forall c \in A. c = \bigvee \{ d \in A^C \mid d \leq c\}$.
\end{definition}

We let $A^C \subseteq A$ denote the set of compact elements of ${\mathbb C}$. }

%We considered the LUBs of possibly infinite sets just for the sake of simplicity: 
%our proposal would fit also the finite case.
%
%Obviously, Ls also have the greatest lower bound for any subset $Y \subseteq A$.
%In the following we fix a BL ${\mathbb L} = \langle A, \leq, \monid \rangle$.

%\begin{definition}[compact elements]
%An element $a \in A$ is compact (or finite) if whenever $a \leq \bigvee Y$ there exists a finite subset
%$X \subseteq Y$ such that $a \leq \bigvee X$.
%%
%%Let $A^C \subseteq A$ be the set of compact elements of ${\mathbb C}$.
%%Then ${\mathbb C}$ is algebraic if $\forall c \in A. c = \bigvee \{ d \in A^C \mid d \leq c\}$.
%\end{definition}


%Note that for complete lattices the definition of compactness given above coincides with the one using
%directed subsets. It will be easier to generalize it, though, to compactness with respect to the monoidal operator (see Def.~\ref{def:compactness}).
%
%We let $A^C \subseteq A$ denote the set of compact elements of ${\mathbb C}$. Note however
%that $A^C$ might be trivial: indeed, in the the segment $[0, 1]$ of the reals
%with the usual order, only $0$ is a compact element. As we are going to see, the situation for the soft paradigm
%can be more nuanced.
%\marginpar{is algebraicity needed?}
%

\begin{definition}[monoid]
	A (commutative) monoid is a triple
	$\langle A, \monop, \monid \rangle$ such that $A$ is a set, $\monop: A \times A \rightarrow A$ is
	a commutative and associative function, and $\monid \in A$ is the \emph{identity} element,
	namely, $\forall a \in A. a \monop \monid = a$. % where $\monid \in A$ is the \emph{identity} element.
	
	A partially ordered (semi-lattice) monoid is a 4-tuple
	$\langle A, \leq, \monop, \monid \rangle$ such that 	
	$\langle A, \leq \rangle$ is a PO (SL) and $\langle A, \monop, \monid \rangle$ a monoid.
	
%	\noindent
%	A partially ordered monoid is monotone if 
%	satisfying
% 	A weakly ordered monoid is ordered if 
%	\begin{itemize}
%		\item $\forall a, b, c \in A. a \leq b \implies c \monop a \leq c \monop b$.
%	\end{itemize}
%	A semi-lattice monoid is an ordered (weakly so, respectively) monoid 
%	such that its underlying PO is an SL. 
\end{definition}

As usual, we use the infix notation: $a \monop b$ stands for $\monop(a,b)$.
\comment{The monoidal operator can be defined for any multi-set: it is given 
for a family of elements $a_i \in A$ indexed over a finite, non-empty
set $I$, and it is denoted by
$\bigotimes_{i \in I} a_i$.
%
If for an index set $I$ the $a_i$'s are different,
we write $\bigotimes S$ instead of $\bigotimes_{i \in I} a_i$
for the set $S = \{a_i \mid i \in I\}$.
%
Conventionally, we denote $\bigotimes \emptyset = \bot$.}

\begin{definition}[distributivity]
\label{dist}
Let $\langle A, \leq, \monop, \monid \rangle$ be a semi-lattice monoid.
It is distributive if
	for  any  non-empty finite  $X \subseteq A$
	\begin{itemize}
		\item $\forall a \in A.\,  a \monop  \bigvee X = \bigvee \{a \monop x \mid x \in X\}$.
	\end{itemize}

\end{definition}

Note that distributivity implies that $\otimes$ is monotone with respect to $\leq$.
\begin{remark}
% i.e., it holds
%	\begin{itemize}
%		%\item 
%		$\forall a, b, c \in A. a \leq b \implies c \monop a \leq c \monop b$.
%	\end{itemize}

	It is almost straightforward to show that our proposal encompasses many other formalisms in the literature.
	Indeed, distributive semi-lattice monoids are \emph{tropical} semirings (also known as dioids), 
	namely, semirings with an idempotent sum operator $a \oplus b$, which in our formalism is obtained as
	$\bigvee \{a, b\}$.
	% that is idempotent.
	%~\cite{tropical}. 
	If $\monid$ is the top of the SL we end up 
	in \emph{absorptive} semirings~\cite{golanShort}, 
	which are known as $c$-semirings 
	in the soft constraint jargon~\cite{jacm97} (see e.g.~\cite{ecai06} for a brief survey on residuation 
	for such semirings).
	%
%	%Indeed, it is precisely the lack of the latter requirement on $\monid$ that makes ReSLs suitable for modelling bipolar 
%	%preferences:
	Note that requiring the monotonicity of $\otimes$ and imposing $\monid$ to be the top of the partial order
	means that preferences are negative, i.e., 
	that it holds $\forall a, b \in A. a \monop b \leq a$.
%	%, and that $\forall a, b \in A. a \leq b \implies \monid = b \odiv a$.
%	%
\end{remark}

\begin{example}
Given a (possibly infinite) set $V$ of variables, two semi-lattice monoids are going to play a key role in the following sections.

The first one is the semi-lattice monoid 
$\mathbb{M}(V) = \langle 2^V_{fin}, \subseteq, \cup, \emptyset \rangle$
of finite sub-sets of $V$, with the usual order given by sub-set inclusion.
%\todo{come in un commento nel seguiro, non trovo spiegazione della $f$ in $2^V_f$}

For the second one, we start by defining the support of an endofunction $f\colon V \to V$ as the set $sv(f) = \{ x \in V \mid f(x)\neq x \}$ and
$F(V)$ as the set of functions $f\colon V \to V$ with finite support.
The semi-lattice monoid of interest is  $\mathbb{F}(V) = \langle F(V), id, \circ, \iota \rangle$ where 
$\iota$ is the identity function,  $\circ$ is function composition and $id$ is the discrete ordering on $F(V)$.
\end{example}

\bigskip
%
% COMMENTATO DA FILIPPO
%
%\begin{remark}
%The developments reported in Section~\ref{cypo} could be stated also for \emph{infinite} subsets 
%and for functions whose support is not necessarily finite. More on this later on.
%\end{remark}

%$a, b \in A$.
%
%The monoidal operator can be defined for any finite multiset: it is given for a family of elements
%$a_i \in A$ indexed over a finite set $I$, and it is denoted by
%$\bigotimes_{i \in I} a_i$.
%%
%Whenever for an index $I$ all the $a_i$'s are different,
%we simply write $\bigotimes S$ instead of $\bigotimes_{i \in I} a_i$
%for the set $S = \{a_i \mid i \in I\}$.
%%
%Conventionally, we will also usually denote $\bigotimes \emptyset = \top$.
%
%%smallskip
%%In the following we fix a IM ${\mathbb M} = \langle A, \monop, \monid \rangle$.
%
%We now move our attention to the domain of values we are going to consider.

\subsection{Remarks on Residuation}\label{sec:ror}
It is often needed to be able to ``remove'' part of a preference, due e.g. 
to the non-monotone nature of the language at hand
for manipulating constraints. 
%
The structure of our choice is given by residuated monoids~\cite{golanShort}. 
%
They introduce a new operator $\odiv$, which represents a ``weak'' (due to the presence of partial orders) inverse of $\otimes$.

\begin{definition}[residuation]\label{def:repo}
	A residuated monoid (RePO) is a 5-tuple $\langle A, \leq, \monop, \odiv, \monid \rangle$ such that
	$\langle A, \leq, \monop, \monid \rangle$ is a partially ordered monoid and
	$\odiv: A \times A \rightarrow A$ is a function satisfying 
	\begin{itemize}
		\item $\forall a, b, c \in A. b \monop c \leq a \iff c \leq a \odiv b$.
	\end{itemize}
	An ReSL is an RePO such that the underlying PO is a SL.
\end{definition}

%In the following sections on oft CCP, we will often use absorptive RePOs, i.e., such that 
%	\begin{itemize}
%		\item[] $\forall a, \in A. a \leq 1$.
%	\end{itemize}
%
%However, 

%Residuation is monotone on the first argument: 
%$\forall a, b, c \in A. a \leq b \implies a \odiv c \leq b \odiv c$.
%Among other things, n
In order to confirm the intuition about weak inverses,
Lemma~\ref{lemma:residuation} below precisely states that residuation conveys the meaning of 
an approximated form of subtraction.
% which can be used to remove a constraint from another.
% operator.

\begin{lemma}\label{lemma:residuation}
	Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an RePO.
	Then
	\begin{itemize}
		\item $\forall a, b \in A.\, a \odiv b = \bigvee \{ c \mid b \monop c \leq a\}$,
	\end{itemize}
\end{lemma}

\shortNoProof{
\begin{proof}
	By definition $a \odiv b$ is an upper bound of 
	$\{ c \mid b \monop c \leq a\}$ and $b \monop (a \odiv b) \leq a$.
	%
%	The latter property ensures the monotonicity of $\odiv$ on the first argument,
%	since by definition $a \odiv c \leq b \odiv c$ iff $c \monop (a \odiv c) \leq b$.
	%
%	As for the  monotonicity of $\monop$, it suffices to note that by definition
%	$a \leq (b \monop a) \odiv b$ and also by definition $c \monop a \leq c \monop b$ iff 
%	$a \leq (c \monop b) \odiv c$.
\qed
\end{proof}
}

%Note that by commutativity, 
%Thus $\monop$ is monotone (on both arguments), and
%the underlying monoid is ordered.
%while $\odiv$  is clearly anti-monotone on the second argument: 

In order to ease the verification of the algebraic structure, it is often needed
a characterisation of residuation via simpler properties,
as the ones given below.

\begin{lemma}
\label{mono}
Let $\langle A, \leq, \monop, \monid \rangle$ be a partially ordered monoid  and
	$\odiv: A \times A \rightarrow A$ a function. Then $\langle A, \leq, \monop, \odiv, \monid \rangle$ is an RePO if and only if
	\begin{itemize}
		\item $\forall a, b \in A. b \monop (a \odiv b) \leq a \leq (b \monop a) \odiv b$,
		\item $\forall a, b, c \in A.\, a \leq b \implies a \otimes c \leq b \otimes c$ and $a\odiv c \leq b \odiv c$.
\end{itemize}
\end{lemma}

\shortNoProof{
\begin{proof} ($\Longrightarrow$)
The first item is immediate. Now, let $a \leq b$. Since $b \leq (b \otimes c) \odiv c$ and 
$c \otimes (a \odiv c) \leq a$, the second item follows.

($\Longleftarrow$)
Using the monotonicity of $\odiv$ from $b \monop c \leq a$ we get
 $(b \monop c) \odiv b \leq a \odiv b$, and by the first item
 $c \leq a \odiv b$.
 %
 From the latter by the monotonicity of $\otimes$ we get
 $b \otimes c \leq b \otimes (a \odiv b)$, and by the first item
 $b \monop c \leq a$.
 %
%	Immediate: $a \odiv b$ is an upper bound of 
%	$\{ c \mid b \monop c \leq a\}$ and $b \monop (a \odiv b) \leq a$.
	%
%	The latter property ensures the monotonicity of $\odiv$ on the first argument,
%	since by definition $a \odiv c \leq b \odiv c$ iff $c \monop (a \odiv c) \leq b$.
	%
%	As for the  monotonicity of $\monop$, it suffices to note that by definition
%	$a \leq (b \monop a) \odiv b$ and also by definition $c \monop a \leq c \monop b$ iff 
%	$a \leq (c \monop b) \odiv c$.
\qed
\end{proof}
}

It is easy to show that in any RePO the $\odiv$ operator is also anti-monotone on the second argument, i.e., 
$\forall a, b, c \in A.\, a\leq b \implies  c\odiv b \leq c \odiv a$.
%
Other properties are also straightforward, such as 
$\forall a\in A. \monid \leq a \odiv a$, which in turn implies 
that $\forall a\in A. a \monop (a \odiv a) = a$ and
%
%, and \emph{iii)} $a \odiv (b \monop c) = (a \odiv b) \odiv c$.
%should $\monop$ be idempotent, $b \leq a$ implies $a \odiv b = a$.
%
$\forall a, b \in A. a < b \implies \monid \not \leq a \odiv b$, where
$a < b$ means $a \leq b$ and $a \neq b$.
%
%Residuation is monotone on the first argument:
%$\forall a, b, c \in A. a \leq b \implies a \odiv c \leq b \odiv c$.
%%falsa and if $b \leq a$, then $a \odiv b = \monid$. For more properties of residuation we refer to \cite[Table~4.1]{resbook}.
%
%
The latter fact suggests the definition below, which identifies sub-classes 
of residuated monoids that are suitable for an easier manipulation
of constraints (see e.g.~\cite{ecai06}).

\begin{definition}[localisation / invertibility]
	An RePO $\langle A, \leq, \monop, \odiv, \monid \rangle$ is
	\begin{itemize}
		\item
		\emph{localised} if $\forall a, b \in A. a \leq b \implies a \odiv b \leq \monid$;
		\item
		\emph{invertible} if $\forall a, b \in A. a \leq b \implies b \monop (a \odiv b) = a$.
	\end{itemize}
\end{definition}

Note that if a RePO is localised then $\forall a \in A. a \odiv a = \monid$.
%\marginpar{all RePO are localized?}

\begin{remark}
	Note that the equivalence $a \otimes ((a \otimes b) \odiv a) = a \otimes b$ always holds, even if the 
	underlying RePO is not invertible. Indeed, we have by definition $a \otimes ((a \otimes b) \odiv a) 
	\leq a \otimes b$, as $(a \otimes b) \odiv a \leq b$. We must check that $a \otimes ((a \otimes b) 
	\odiv a) \leq a \otimes b \iff b \leq ((a \otimes b) \odiv a) \iff a \otimes b \leq a \otimes b$,
	which is trivially true.
\end{remark}

\begin{remark}\label{rmk:soft}
	Some well-known structures used for soft constraints are the 
	%\emph{Boolean} ($\langle \{\mathit{false},\mathit{true}\}, \mathit{false} \leq \mathit{true}, \wedge, \mathit{false}, \mathit{true}\rangle$), 
	\emph{Fuzzy} ($\langle [0,1], \leq,$ $\min, 1 \rangle$), \emph{Probabilistic} ($\langle [0,1], \leq,\allowbreak\times, 1 \rangle$), 
	and \emph{Tropical}   ($\langle \mathbb{R}^+, \geq, +, 0 \rangle$) semirings, for $\geq$ the inverse of the standard order 
	(thus $0$ the top of the SL). In all these cases the underlying monoids 
	are both invertible and localised, thus
	%
	the $\odiv$ operator can be also used to
	(partially) relax constraints (see again~\cite{ecai06}).
\end{remark}

Moving to ReSLs, next lemma ensures that residuation implies distributivity.

\begin{lemma}
	\label{dist2}
	Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an ReSL.
	Then the underlying SL is distributive.
%	 and $X \subseteq A$ a finite set.
%	Then
%	\begin{itemize}
%		\item $\forall a \in A.\, a \monop  \bigvee X = \bigvee \{a \monop x \mid x \in X\}$.
%	\end{itemize}
\end{lemma}

\shortNoProof{
\begin{proof} Let $X \subseteq A$ be a finite non-empty set. 	
	%\paragraph{$\bigvee \{a \monop x \mid x \in X\} \leq a \monop  \bigvee X$}
	\[\forall x \in X.\, x \leq \bigvee X %\implies %\forall x \in X.\, x \leq (a \monop \bigvee X) \odiv a \implies\]
	\implies \forall x \in X.\, a \monop x \leq a \monop \bigvee X \implies \bigvee \{a \monop x \mid x \in X\} \leq a \monop  \bigvee X .\]

	%So, let us assume that $X$ is inhabited.
	%\paragraph{$a \monop  \bigvee X \leq \bigvee \{a \monop x \mid x \in X\}$}
	\[\forall y \in X.\, a \monop y \leq \bigvee \{a \monop x \mid x \in X\} \implies 
	\forall y \in X.\, y \leq (\bigvee \{a \monop x \mid x \in X\}) \odiv a \implies\] 
	\[ \implies \bigvee X \leq (\bigvee \{a \monop x \mid x \in X\}) \odiv a \implies 
	a \monop \bigvee X \leq \bigvee \{a \monop x \mid x \in X\} .\] 
\qed
\end{proof}
}
%Note that the proof does not require that $\otimes$ is monotone, which is thus a derived property.
%
Distributivity holds also for the empty set and for infinite sets, if the necessary LUBs exist.
%
Instead, it holds only partially for $\odiv$: this follows directly from the monotonicity of $\odiv$ on the first argument, 
since it implies that $x \odiv a \leq \bigvee X \odiv a$ for all $x \in X$.

\begin{lemma}
	\label{distodiv}
	Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an ReSL and $X \subseteq A$ a finite non-empty set. Then 
	\begin{itemize}
		\item $\forall a \in A.\, \bigvee \{ x \odiv a \mid x \in X \} \leq \bigvee X \odiv a$
	\end{itemize}	
\end{lemma}

\shortNoProof{
\begin{proof}
Straightforward, since by the monotonicity of $\odiv$ in the first argument (Lemma~\ref{mono}) we get
% \[\forall x \in X.\,a \otimes (x \odiv a) \leq x \implies\]
 %\[\forall x \in X.\,a \otimes (x \odiv a) \leq \bigvee X \implies\]
 $\forall x \in X.\,x \odiv a \leq \bigvee X \odiv a$, which implies
 $\bigvee \{ x \odiv a \mid x \in X\} \leq \bigvee X \odiv a$.
% \[\]
\qed
\end{proof}
}

%\begin{remark}
Also this inequation holds for the empty set and for infinite sets, if the necessary LUBs exist.
%
Moreover, it also holds that $\bigvee \{ a \odiv x \mid x \in X \} \geq a \odiv \bigvee X$, since $\odiv$ is anti-monotone on the second argument.
%\end{remark}

\begin{proposition}\label{reabs}
	Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an ReSL. The following are equivalent
	\begin{enumerate}
		\item $\forall a \in A.\, a \leq \1$
		\item $\forall a \in A.\, \1 \odiv a = \1$		
		\item $\forall a, b \in A.\, a \leq b \implies b \odiv a = \1$
	\end{enumerate}	
\end{proposition}

\shortNoProof
{
\begin{proof}
Note that $1$ immediately implies both $2$ and $3$, since by definition $a \leq b$ implies $\1 \leq b \odiv a$
and $\1$ is the top of the partial order.

For the second step, first note that both properties implies that $\1 \odiv a \leq \1$ for all $a \in A$. This is immediate
for $2$. As for $3$, %let us assume that $b \odiv a \leq \1$, and
consider $b = \bigvee \{\1, a \}$. By Lemma~\ref{distodiv} we have that
$\bigvee \{\1 \odiv a, a \odiv a\} \leq \bigvee \{\1, a \} \odiv a$. 
Hence, $\1 \odiv a \leq \1$ for all $a \in A$, and the result follows.

Finally, note that $\1 \odiv a \leq \1$ for all $a \in A$ implies that $\1 \odiv (\1 \odiv c) \leq \1$ for all $c \in A$, 
and since it always holds
that $c \leq \1 \odiv (\1 \odiv c)$, then $3$ implies $1$.
\qed
\end{proof}
}

%\begin{remark}
%In general, given an ReSL $\mathbb{M} = \langle A, \leq, \otimes, \odiv, \monid \rangle$, if $A \subseteq B$ such that $B(\otimes)$ is a group and $\odiv$
% is the inverse of $\otimes$, then $\mathbb{M}$ is fully $%\odiv$-distributive, which follows from $\mathbb{M}$ is distributive for $\otimes$. \\
%Consider, for istance, $\mathbb{M} = \langle \{0,...,5\},\geq,\oplus,\ominus,0 \rangle$, where $\oplus$ and $\ominus$ are the bounded sum and subtraction (e.g. $2 \oplus 4 = 5$, $2 \ominus 4 = 0$): 
%it is clear that, in this case, distributivity holds for $\ominus$, as long as $a \geq b \implies b \ominus a = 0$. \\
%\todo{un esempio dove $\odiv$ non distribuisce}
%In the following example it is shown that distributivity for $\odiv$ could hold partially, since we choose a residuation operator which is not the inverse of $\otimes$.
%\end{remark}

\begin{remark}\label{rmk:softUnit}
The proposition above provides an important characterisation for all absorptive ReSLs, including all those mentioned in Remark~\ref{rmk:soft}.
\end{remark}

There are some important classes of ReSLs  such that $\odiv$ is easily proved to be distributive in the first argument,
while it is not so with respect to the second argument, not even in the absorptive case.

\begin{lemma}
	\label{distodiv2}
	Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an ReSL such that $\langle A, \leq \rangle$ is a total order and $X \subseteq A$ a finite non-empty set. Then 
	\begin{itemize}
		\item $\forall a \in A.\, \bigvee \{ x \odiv a \mid x \in X \} = \bigvee X \odiv a$
	\end{itemize}	
\end{lemma}

\shortNoProof{
\begin{proof}
If $\langle A, \leq \rangle$ is a total order and $X$ is finite and non-empty we have that $\bigvee X \in X$, and since $\odiv$ 
is monotone on the first argument (see Lemma~\ref{mono}) the result follows.
\qed
\end{proof}
}

\begin{example}
\label{nodist2}
%Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an ReSL such that $\langle A, \leq \rangle$ is a total order.
%Then, it holds that $\bigvee \{ x \odiv a \mid x \in X \} = \bigvee X \odiv a$ for all elements $a$ and (finite, non-empty) subsets $X$.
%In fact, if $\langle A, \leq \rangle$ is a total order and $X$ is finite and non-empty we have that $\bigvee X \in X$, and since $\odiv$ 
%is monotone on the first argument (see Lemma~\ref{mono}), the result follows.
%
Let $n$ be a positive integer and $[n] = \{0, \ldots, n\}$ the segment of integers from $0$ to $n$. We can now define the (bounded) monoid $\mathbb{M}_n$ 
as the tuple $\langle [n], \geq, \oplus, \ominus, 0 \rangle$, where $\oplus$ and $\ominus$ are the bounded sum and subtraction, 
which are given as $m\oplus p = min\{n, m+p\}$ and $m\ominus p = max\{0,m-p\}$.

Now, it can be shown that $\mathbb{M}_n$ is an absorptive ReSL, and since it is a total order,
$\ominus$ is  distributive on the first argument.
%
However, in general it is not distributive on the second one. Consider an integer $m$ such that 
$m \neq n$ and the set $\{m, m+1\}$:
we then have that $(m+1) \ominus \bigvee\{m, m+1\} = 1$,
while instead $\bigvee\{(m+1) \ominus m, (m+1) \ominus (m+1)\} = 0$.
\end{example}

\comment{
\begin{example}
Given $A = \{0,a,b,c,d,e\}$, consider the following partial order:
	\begin{center}
		\begin{tikzpicture}
			\node (top) at (0,0)  {$0$};
			\node (a) [below of= top] {$a$};
			\node [below left of=a] (left) {$b$};
			\node [below right of=a] (right) {$c$};
			\node (d) [below right of=left] {$d$};
			\node (e) [below of=d] {$e$};
			\draw [thick] (top) -- (a);
			\draw [thick] (a) -- (left);
			\draw [thick] (a) -- (right);
			\draw [thick] (left) -- (d);
			\draw [thick] (right) -- (d);
			\draw [thick] (d) -- (e);
		\end{tikzpicture}
	\end{center}
and $\mathbb{M} = \langle A, \geq, \otimes, \odiv, 0 \rangle$, where $\otimes$ and $\odiv$ are defined as follows:
\begin{center}
	\begin{tabular}{@{} *{7}{c} @{}}
	\\ $\otimes$ \ & 0 \ & a \ & b \ & c \ & d \ & e
	\\ 0 \ & 0 \ & a \ & b \ & c \ & d \ & e
	\\ a \ & a \ & b \ & c \ & d \ & e \ & f
	\\ b \ & b \ & c \ & d \ & d \ & e \ & e
	\\ c \ & c \ & d \ & d \ & d \ & e \ & e
	\\ d \ & d \ & e \ & e \ & e \ & e \ & e
	\\ e \ & e \ & e \ & e \ & e \ & e \ & e
	\end{tabular}
\\	
	\begin{tabular}{@{} *{7}{c} @{}}
	\\ $\odiv$ \ & 0 \ & a \ & b \ & c \ & d \ & e
	\\ 0 \ & 0 \ & 0 \ & 0 \ & 0 \ & 0 \ & 0
	\\ a \ & a \ & 0 \ & 0 \ & 0 \ & 0 \ & 0
	\\ b \ & b \ & a \ & 0 \ & 0 \ & 0 \ & 0
	\\ c \ & c \ & a \ & 0 \ & 0 \ & 0 \ & 0
	\\ d \ & d \ & c \ & c \ & b \ & 0 \ & 0
	\\ e \ & e \ & d \ & b \ & c \ & a \ & 0
	\end{tabular}
\end{center}
Then $\mathbb{M}$ is an absorptive ReSL with $0$ the top of the partial order, since it behaves as the ReSL in the example above, except for $b$ and $c$: thus, in this case, $\bigvee \{b,c\} = a$. \\
It's now easy to show that $\odiv$ is not distributive for the first argument: $\bigvee{b \odiv a, c \odiv a} = a$ and $\bigvee\{b,c\} \odiv a = a \odiv a = 0$.
\end{example}

%
%We can proove $\bigvee X \odiv a = \bigvee \{ x \odiv a \mid x \in X \}$ under the following hypotesis.
%
%\begin{lemma}
%	\label{distodiv2}
%	Let $\langle A, \leq, \monop, \odiv, \monid \rangle$ be an ReSL.
%	If $a \otimes (x \odiv a) = x = (a \otimes x) \odiv a$, then $\bigvee X \odiv a = \bigvee \{ x \odiv a \mid x \in X \}$.
%\end{lemma}
%
%\begin{proof}
% \[\bigvee X = \bigvee \{ a \otimes (x \odiv a) \mid x \in X \} \implies\]
% \[\bigvee X = a \otimes \bigvee \{ x \odiv a \mid x \in X \} \implies\]
% \[\bigvee X \odiv a = (a \otimes \bigvee \{ x \odiv a \mid x \in X\}) \odiv a \implies\]
% \[\bigvee X \odiv a = \bigvee \{ x \odiv a \mid x \in X\}.\]
% \[\]
%\end{proof}
%
%[Qui avevi fatto una correzione, mettendo nell'ipotesi una disuguaglianza al posto della seconda uguaglianza, ma in quel modo non potrei provare l'ultimo passaggio, quindi te la rispedisco cos\`{i}. 
%Magari ne discutiamo quando ci vediamo.]
%\\

%Distributivity over $\bigvee$ implies that $\monop$ is
%monotone in both arguments.
%%as well as $\forall a \in A. a \monop \bot = \bot$.

%%
%In the following, we fix a BLIM ${\mathbb S} = \langle A, \leq, \monop \rangle$.
%%
%The next step is to provide a suitable notion of infinite composition. The definition below is taken from~\cite{CLIM}
%(but see also~\cite[p.~42]{golan}).
%
%\begin{definition}[infinite composition]
%Let $I$ be a (possibly countable) set of indexes. Then, the composition $\bigotimes_{i \in I} a_i$
%is defined as $\bigvee_{J \subseteq I} \bigotimes_{j \in J} a_j$ for all finite subsets $J$.
%\end{definition}

%%\marginpar{distributivity wrt. $\vee$ or wrt. $\wedge$ coincide?}
%Thanks to distributivity, we can show that
%$\bigotimes$ is monotone, i.e., $\forall j \in I. a_j \leq b_j \implies
%\bigotimes_{i \in I} a_i \leq \bigotimes_{i \in I} b_i$.

%We now extends the notion of compactness.
%
%\begin{definition}[$\monop$-compact elements]\label{def:compactness}
%An element $a \in A$ is $\monop$-compact (or $\monop$-finite) if whenever $a \leq \bigotimes_{i \in I} a_i$
%then there exists a finite subset $J \subseteq I$ such that $a \leq \bigotimes_{j \in J} a_j$.
%
%Let $A^\monop \subseteq A$ be the set of $\monop$-compact elements of ${\mathbb S}$. Then ${\mathbb S}$ is
%$\monop$-algebraic if $\forall c \in A. c = \bigotimes \{ d \in A^\monop \mid d \leq c\}$.
%\marginpar{now $\monop$-algebraicity is incorrect}
%\end{definition}

%We let $A^\monop \subseteq A$ denote the set of $\monop$-compact elements of ${\mathbb S}$.
%%
%It is easy to show that a compact element is also $\monop$-compact.
%%
%Indeed, the latter notion is definitively more flexible.
%%
%Let us consider again the segment $[0, 1]$ of the reals, yet now with the inverse of the usual order (as used
%in the probabilistic SCPs). Instead of the LUB, an alternative monoidal
%product can be just the multiplication.
%%
%Since any infinite multiplication tends to $0$, then all the elements are
%$\monop$-compact, except the top element itself, that is, precisely $0$.
%\marginpar{is $\monop$-algebraicity needed?}
}

\comment{
\subsection{On residuation and semirings}

We now consider \emph{semirings} equipped with a partial order~\cite[Chapter~2]{golanShort}.

\begin{definition}[semirings]
	A (commutative) semiring is a 5-tuple
	$\langle A, \monop, \monop, \monid, \1 \rangle$ such that $\langle A, \monop, \monid \rangle$
	and $\langle A, \monop, \1 \rangle$ are (commutative) monoids
	satisfying
	\begin{itemize}
		\item $\forall a \in A. a \monop \monid = \monid$
		\item $\forall a, b, c \in A. a \monop (b \monop c) = (a\monop b) \monop (a \monop c)$
	\end{itemize}
	An ordered semiring is a 6-tuple
	$\langle A, \leq, \monop, \monop, \monid, \1 \rangle$
        such that  $\langle A, \leq, \monop, \monid \rangle$ is an ordered monoid and 
   	$\langle A, \monop, \monop, \monid, \1 \rangle$ a semiring satisfying
	\begin{itemize}
			\item $\forall a, b, c \in A. a \leq b \wedge \monid\leq c \implies c \monop a \leq c \monop b$
	\end{itemize}
\end{definition}

We often use an infix notation, as $a \monop b$ for $\monop(a,b)$.


[A QUESTO PUNTO BISOGNA VEDERE QUALI DI QUESTE TRE PROPRIETA'
DELL'ordered SEMIRING POSSONO DISCENDERE DA QUELLE DELLA RESIDUAZIONE,
IN MODO DA AVERE GLI EQUIVALENTI DEI LEMMA 3 E 4]

[ MA 1 MI SERVE A QUALCOSA?]

\begin{definition}[residuation, II]
	A residuated semiring (ReS) is a 7-tuple $\langle A, \leq, \monop, \odiv, \monop, \monid, \1 \rangle$
	such that	$\langle A, \leq, \monop, \odiv, \monid \rangle$
	 is a residuated monoid and $\langle A, \leq, \monop, \monop, \monid, \1 \rangle$ an ordered semiring,
	  satisfying 
	\begin{itemize}
            ????
	\end{itemize}
	A residuated SSL (ReSSL) is an ReS such that the underlying PO is a SL.
\end{definition}

[COSA PUO' SERVIRE COME ASSIOMA?]

%In the following sections on oft CCP, we will often use absorptive RePOs, i.e., such that 
%	\begin{itemize}
%		\item[] $\forall a, \in A. a \leq 1$.
%	\end{itemize}
%
%However, 
}

\comment{
Indeed, there are many classes of absorptive and idempotent ReSLs such that $\odiv$ 
is not distributive in either arguments.

\begin{example}
\label{notdistr}
First of all, note that a complete sup-lattice $\langle A, \leq \langle$ (i.e., admitting a sup for all subsets 
of $A$) can be turned into a ReSL. Indeed, $\otimes$ is just the meet, so we have that 

\begin{itemize}
\item $a \otimes b = \bigvee \{c \mid c \leq a \wedge c \leq b\}$
\item $a \odiv b = \bigvee \{c \mid c \otimes b \leq a\}$
\end{itemize}


$$x \otimes y = \bigg \{\begin{array}{ll}
	\1 & \mbox{ if } y \leq x \\
	x & \mbox{ if } y = \1 \\
	\bot & \ otherwise
	\end{array}$$

both meetand divis




the bounded sum $\oplus$ is here idempotent, $0$ is still 
the identity. We can make it int
of three otherwise unrelated elements, 
so that for all elements $x$ we have $x \otimes x = \1 \otimes x = x$ \
and furthermore $a \otimes b = a \otimes c = b \otimes c  = \1$.

We now add the bottom element $\bot$, in order to obtain a complete lattice.
Then $\otimes$ is extended in the expected way, so that $\bot$ is absorbing.
%
The resulting semi-lattice monoid is absorptive and residuated, with $\odiv$ defined as

$$x \odiv y = \bigg \{\begin{array}{ll}
	\1 & \mbox{ if } y \leq x \\
	x & \mbox{ if } y = \1 \\
	\bot & \ otherwise
	\end{array}$$
%
Thus, $\odiv$ does not distribute, since 
$\bigvee \{a \odiv c, b \odiv c\}  = \bot < \1 = \1 \odiv c = \bigvee \{a, b\} \odiv c$.
\end{example}
}

\comment{
\begin{example}
\label{notdistr}
Let us consider the monoid $S = \langle \{p,u,n,t\}, \otimes_s, u \rangle$ (with $t$ the top 
of three otherwise unrelated elements): 
$p$ and $n$ intuitively represent the sign of an integer, $t$ tells us that 
the sign cannot be determined, $u$ is the zero
and $\otimes_s$ (which is idempotent) tells us the sign of the addition of two integers, so that 
for all elements $x$ we have
\[x \otimes_s x = u \otimes_s x = x \mbox{  and  } t \otimes_s x = p \otimes_s n = t\]
%
We now add the bottom, in order to obtain a complete lattice.
The $\otimes_s$ is extended in the expected way,  so that $\bot$ is absorbing.
%
Intuitively, $\bot$ states that an element is unsigned:
a pattern the reader familiar with abstract interpretation formalisms will recognise.

The resulting semi-lattice monoid is residuated, with $\odiv$ defined as

$$x \odiv y = \bigg \{\begin{array}{ll}
	t & y \leq x \\
	\bot & \ otherwise
	\end{array}$$
%
Thus, $\odiv$ does not distribute, since 
$\bigvee \{p \odiv n, u \odiv n\}  = \bot < \bigvee \{p, u\} \odiv n = t \odiv n = t$.
\end{example}
}

\section{An Alternative Proposal for Costraint Manipulation}
\label{newpro}

This section presents our personal take on polyadic algebras for ordered monoids:
the standard axiomatisation of e.g.~\cite{sagi2013} has been completely 
reworked, in order to be adapted to the constraints formalism.
%
We close the section by offering some preliminary insights on 
the laws for polyadic operators in residuated monoids.

\subsection{Cylindric and Polyadic Operators for Ordered Monoids}
\label{cypo}
We now introduce two families of operators 
%(cylindric and polyadic ones) 
that will be used
for modelling variables hiding and substitution, which represent
key features in languages for manipulating constraints.
%
One is a well-known abstraction for existential quantifiers,
the other an axiomatisation of the notion of
substitution, and it is proposed as a weaker  alternative 
to diagonals~\cite{popl91}, the standard tool for modelling 
equivalence in constraint programming.\footnote{``Weaker 
alternative'' here means that diagonals allow for axiomatising
substitutions at the expenses of working with complete
partial orders: see e.g.~\cite[Definition 11]{jlamp17}.}
%

\comment{\smallskip
Our first step is the introduction of a technical notion that allows for 
factorising the common properties in the definition of the two families of operators.

\begin{definition}[pomonoid action]
\label{pomo}
Let $\mathbb{M} = \langle A, \leq, \monop, \monid \rangle$ be a partially ordered monoid and $\mathbb{P} = \langle S, \leq \rangle$ a partial order.
A pomonoid action of $\mathbb{M}$ on $\mathbb{P}$ is a function $\phi: A \times S \rightarrow S$ such that
	\begin{itemize}
	     \item $\forall s \in S.\ \phi(\monid, s) = s$,
         \item $\forall a, b \in A,\ s \in S.\ \phi(a, \phi(b, s)) = \phi(a \otimes b, s)$,
         \item $\forall a, b \in A,\ s, t \in S.\ a \leq b\, \wedge\, s \leq t \implies \phi(a, s) 
         \leq \phi (b, t)$.
            % \item $\forall a, b \in A,\ s \in S.\ a \leq b \implies \phi(a, s) \leq \phi (b, s)$.
	\end{itemize}
\end{definition}

The first two requirements just state
that $\phi$ is a monoid action of $\mathbb{M}$ on $S$, while the latter states that $\phi$ is monotone. Sometimes, we say that $\mathbb{P}$ is an $\mathbb{M}$-PO.}

\subsubsection{Cylindric operators.}
We fix a partially ordered monoid $\mathbb{S} = \langle A, \leq, \monop, \monid \rangle$
and a set $V$ of variables, and we then define a family of cylindric operators axiomatising existential quantifiers.

\begin{definition}[cylindrification]\label{cyli}
	A cylindric operator $\exists$ over $\mathbb{S}$ and $V$ ia family of monotone operators
	$\exists_x : A \rightarrow A$ indexed by elements in V such that for all 
	$a, b \in A$ and $x, y \in V$
	%\todo{$2_f^V$ non e' stato definito prima e/o f non si sa cosa e' qui}
	\begin{enumerate}
	     \item $a \leq \exists_x a$,
         \item $\exists_x \exists_y a = \exists_y \exists_x a$,
         %\item $\forall a, b \in A.\ X \subseteq Y\wedge a \leq b \implies  \exists(X, a) = \exists(Y, b)$,
	     %\item $\exists(X, \monid) = \monid$,
	     \item $\exists_x (a \monop \exists_x b) = \exists_x a \monop \exists_x b$.
	\end{enumerate}
	
	\noindent Let $a \in A$. The \emph{support} of $a$ is the set of variables 
	$sv(a) = \{ x \mid \exists_x a \neq a\}$. 
	% and the set of unsupported variables of $a$ is the set of variables $uv(a) =  V \setminus sv(a)$.
\end{definition}

%Note that, since by Definition~\ref{pomo} we have $\exists(\emptyset, a) = a$, the requirements of Definition~\ref{cyli} trivially hold 
%whenever $X$ is the empty set.
%
%The first two conditions tell us that $\exists$ is a monoid action of $M(V)$ over $A$. Condition $3$ states
%that $\exists$ is a monotone function. Finally, the last two conditions state how $\exists$ interacts with the 
%monoidal structure on $\mathbb{S}$.
%
%\begin{remark}
%TODO bisogna vedere cosa altro serve, e se qualche propriet\`a \`e derivata.
%Cosa succede se $\mathbb{S}$ \`e un SL? Questo impatta sui LUB in M(V)?
%\end{remark}
%
%
%Note also that $\exists(X, \monid) = \monid$ would be a consequence of monotonicity,
%should $\monid$ be the top element. Also, the support is not necessarily finite.
%Finally, and importantly, note that 
%$X \cap sv(\exists(X, a)) = \emptyset$.

%\smallskip
%In the following, we often use $\exists_X a$ for $\exists(X, a)$, and $\exists_x a$ whenever $X = \{x\}$.

\subsubsection{Polyadic operators.}
We now move to define a family of operators axiomatising substitutions.  
They interact with quantifiers, thus, beside a partially ordered monoid $\mathbb{S}$
and a set $V$ of variables, we fix a cylindric operator $\exists$ over ${\mathbb S}$ and $V$.

As for notation, for a function $\sigma: V \rightarrow V$ and a set $X \subseteq V$, we denote by 
$\sigma \mid_{X}: X \rightarrow V$ the obvious restriction, and
by $\sigma^{c}(X) \subseteq V$ the counter-image of $X$ along $\sigma$.
%~\footnote{We are not going to need the other standard component proposed in the literature , i.e., \emph{diagonals}: a %family of elements $d_{x, y} \in A$ indexed by pairs of elements in $V$.}


\begin{definition}[polyadification]
	\label{def:poly}
	A polyadic operator $s$ for $\exists$ is a a family of monotone operators $s_\sigma: A \rightarrow A$
	indexed by elements in $F(V)$ such that for all $x \in V$ and $\sigma, \tau\in F(V)$
	\begin{enumerate}
		\item $sv(\sigma) \cap sv(a) = \emptyset \implies s_\sigma a = a$
		\item $\forall a, b \in A.\ s_\sigma(a \monop b) = s_\sigma a \monop s_\sigma b$,
        \item $\forall a \in A.\ \sigma \mid_{sv(a)} = \tau \mid_{sv(a)} \implies s_\sigma a 
        = s_\tau a$,
        \item $\forall a \in A.\ \exists_x s_\sigma a = \begin{cases}
			s_\sigma \exists_y a &\text{if $\sigma^c(x) = \{y\}$}\\
			s_\sigma a &\text{if $\sigma^c(x) = \emptyset$}
			\end{cases}$.				
    \end{enumerate}
\end{definition}

%Clearly item $3$ always holds for an empty $X$.
%
A polyadic operator offers enough structure for modelling variable substitution. 
%
In the following, we fix a polyadic operator $s$ for $\exists$.

\comment{\begin{remark}
The laws are directly adapted from~\cite{sagi2013}, with the exception of $2$, which 
is stated as for a finite non-empty $X \subseteq V$ and $a \in A$
	\begin{itemize}
          \item[\emph{2'}.] $\sigma \mid_{V \setminus X} = \tau \mid_{V \setminus X}
		         \implies \forall a\in A.\ s(\sigma, \exists (X, a)) = s(\tau, \exists (X, a))$.
        \end{itemize}
However, the two formulations are equivalent. Indeed, note that
$\sigma \mid_{V \setminus X} = \tau \mid_{V \setminus X}$ implies 
$\sigma \mid_{sv(a) \setminus X} = \tau \mid_{sv(a) \setminus X}$, 
which in turn implies that 
$\sigma \mid_{\exists (X, a)} = \tau \mid_{\exists (X, a)}$, and 
assuming item $2$ the result follows.
%
For the vice-versa, first of all note that 
$\sigma \mid_{V \setminus X} = \tau \mid_{V \setminus X}$
coincides with $\sigma \mid_{Y \setminus X} = \tau \mid_{Y \setminus X}$
for $Y = sv(\sigma) \cup sv(\tau) \subseteq V$, and that $Y$ is finite
since both $\sigma$ and $\tau$ are finitely supported.
Now, $\sigma \mid_{sv(a)} = \tau \mid_{sv(a)}$ implies that 
$\sigma \mid_{Y \setminus (Y \setminus sv(a))} = \tau \mid_{Y \setminus (Y \setminus sv(a))}$,
thus by $2a$ we have 
$s(\sigma, \exists (Y \setminus sv(a), a)) = s(\tau, \exists (Y \setminus sv(a), a))$.
Since by definition we have $\exists (Y \setminus sv(a), a)) = a$, the result follows.
\end{remark}
}

%\begin{remark}
%Note also that $\sigma(\sigma^{c}(X)) \subseteq X$, so, when restricted to singleton, we have that item %$3$ in Definition~\ref{def:poly} is equivalent to
%\begin{itemize}
%          \item[\emph{3'}.] $\forall a\in A.\ \sigma^{c}(x) = \{y\} \implies \exists_x s_{\sigma} a =  %s_\sigma \exists_y a$,
%          \item[\emph{3''}.] $\forall a\in A.\ \sigma^{c}(x) = \emptyset \implies \exists_x s_{\sigma} %a =  s_\sigma a$.
%\end{itemize}
%\end{remark}

%\noindent As we did for $\exists$, we define the support of $\sigma$ as follows:
%\begin{itemize}
%\item $sv(\sigma) = \bigcap X \subseteq V \mid \sigma(X) \neq X$
%\end{itemize}

\subsection{Properties of Polyadic Operators}
\label{propo}
In this section we just show some facts concerning
polyadic operators: they ensure that indeed these operators 
suitably axiomatise substitutions. 
%
More precisely, we consider a few simple properties that mimic those holding
for substitutions modelled via diagonals, as considered e.g. in~\cite[Lemma~2 and Lemma~3]{jlamp17}.

\begin{definition}[inverse functions]
	\label{def:inverse}
	Let $\sigma \in F(V)$ be \emph{invertible}, i.e., such that $\sigma \mid_{sv(\sigma)}$ is injective.
	Its \emph{inverse} is defined as
	$$\sigma^{-1}(y) = \bigg \{\begin{array}{rl}
	x & \mathit{if} \ y = \sigma(x) \ and \ x \in sv(\sigma) \\
		y & \ otherwise
	\end{array}$$
	%i.e., $\{^y/_x\}(z) = z$ for all $z \neq x$.
	%
Its \emph{injective lifting} is defined as
	$$\sigma_l(y) = \bigg \{\begin{array}{ll}
	\sigma(y) & if \ y \in sv(\sigma) \\
	\sigma^{-1}(y) & \ otherwise
	\end{array}$$
\end{definition}

In other terms, note that an element $y \in \sigma(sv(\sigma))$ can be the image along $\sigma$ of at most two elements: 
should this be the case, one of them is the element itself 
and the other belongs to $sv(\sigma)$.
When defining the inverse, we give precedence to the element in $sv(\sigma)$.
%
Instead, the injective lifting is indeed an injective substitution.

%Polyadic substitution behaves correctly with respect to $\exists$. 
\begin{lemma} \label{lemma:Inv0}
Let $\sigma \in F(V)$ be invertible. Then it holds
\begin{itemize}
\item $sv(\sigma_l) = \sigma(sv(\sigma)) \cup sv(\sigma) = sv(\sigma_l^{-1})$,
\item $sv(\sigma^{-1}) = \sigma(sv(\sigma))$,
\item $sv(\sigma^{-1} \circ \sigma) = \sigma(sv(\sigma)) \setminus sv(\sigma)$,
\item $sv(\sigma \circ \sigma^{-1}) = sv(\sigma) \setminus \sigma(sv(\sigma))$.
\end{itemize}
\end{lemma}

The set-theoretical proofs are immediate and are omitted. Lemma~\ref{lemmaSubs0} helps to ensure that polyadic substitution behaves correctly with respect to $\exists$. 
We recall that the properties below mirror 
those obtained for substitutions via diagonal operators~\cite{jlamp17}.
%\marginpar{leave if needed}

\comment{
\begin{remark}
With respect to the notion of diagonal
operators~\cite{xxx}\todo{Manca citazione}, as e.g. adopted in~\cite{jlamp17}, in the present context they are not defined, since in our RePOs we do not have the $\top$ elements.

DIRE DI PIU'
\end{remark}
}

\begin{lemma}
	\label{lemmaSubs0}
	Let $\sigma \in F(V)$ invertible, $a \in A$, and $x, y \in V$. Then it holds
	\begin{enumerate}
		\item if $sv(\sigma) \cap sv(a) = \{x\}$, then $s_{\sigma} \exists_x a = \exists_x a$
		\item if $\sigma(sv(\sigma)) \cap sv(a) = \emptyset$ then 
		$\exists_x a = \exists_{\sigma(x)} s_{\sigma} a$
		\item if $y \not \in sv(\sigma) \cup sv(\sigma(sv(\sigma)))$ then 
		$s_{\sigma} \exists_y a = \exists_y s_{\sigma} a$
	\end{enumerate}
\end{lemma}
\begin{proof}
Proofs are immediate.
As for item $1$, it is a consequence of $\sigma \mid_{V \setminus sv(\sigma)} = id \mid_{V \setminus sv(\sigma)}$.

Concerning now item $3$, since $y$ satisfies $y \not \in sv(\sigma)$ then $\sigma(y) = y$,
and since additionally $y \not \in \sigma(sv(\sigma))$ it also follows that $\sigma^{c}(y) = y$. Thus
we have that 
$\exists_y s_{\sigma} a = \exists_{\sigma(y)} s_{\sigma} a = s_{\sigma} \exists_{\sigma^{c}(y)} a = s_{\sigma} \exists_y a$,
where the intermediate equality holds by item 3 of Lemma~\ref{lemma:Inv0}.

Let us finally move to item $2$, and let us consider the lifting $\sigma_l$. This can be less parsimoniously described as
%
%$\sigma'$ defined as follows:
$$\sigma_l(y) = \bigg \{\begin{array}{ll}
	\sigma(y) & \ \mathit{if} \ y \in sv(\sigma) \\
	\sigma^{-1}(y) & \ \mathit{if} \ y \in \sigma(sv(\sigma)) \setminus sv(\sigma) \\
	y & \ otherwise
	\end{array}$$
%Then $\sigma'$ is an injective substitution and switches the variables in $sv(\sigma)$ and $\sigma(sv(\sigma))$ in some way. 
By the hypothesis $\sigma(sv(\sigma)) \cap sv(a) = \emptyset$ we have that $\sigma(x) = \sigma^l(x)$,
%
hence it holds $\exists_x a = s_{\sigma} \exists_x a = s_{\sigma_l} \exists_x a$.
%
Now note that $(\sigma_l)^{c}(\sigma(x)) = x$, for $(\sigma_l)^{c}$ the set-theoretical inverse of $\sigma_l$, 
thus again by item $3$ of Definition~\ref{def:poly} we have that $s_{\sigma_l} \exists_x a = \exists_{\sigma(x)} s_{\sigma_l} a = \exists_{\sigma(x)} s_{\sigma} a$.
%As for $2$, let $\tau$ such that $\hat{\tau} \mid_{sv(\hat{\tau})}: Y \rightarrow V$ is bijective, $Im(\hat{\tau} \mid_{sv(\hat{\tau})}) = X$, $\hat{\tau} \mid_{V \setminus sv(\hat{\tau})} = id \mid_{V \setminus sv(\hat{\tau})}$ and let $\hat{\sigma}'$ defined as follows:
%	\begin{itemize}
%		\item $\hat{\sigma}'(y) = \hat{\sigma}(y)$ if $y \not \in Y$
%		\item $\hat{\sigma}'(y) = \hat{\tau}(y)$ otherwise
%	\end{itemize}
%$\hat{\sigma}'$ switches all the variables in $sv(\hat{\sigma})$ and $sv(\hat{\tau})$ in some way. Then we have $s_{\hat{\sigma}} \exists_Y a = s_{\hat{\sigma}'} \exists_Y a$ and $\exists_Y s_{\hat{\sigma}'} \exists_Y a = s_{\hat{\sigma}'} \exists_X \exists_Y a = \exists_X a$
\qed
\end{proof}

%\begin{proof}
%	The proofs are immediate. Consider for instance the most difficult item $3$.
%	If $x=y$ the proof is over. Now, since $w \not \in \{x, y\}$ we have 
%	by definition that
%	$\delta_{x,y} = \exists_w(\delta_{x,w} \monop \delta_{w,y})$.
%	Again by definition $b [^y/_x] = \exists_x(\delta_{x,y} \monop b)$, so that
%	$\exists_w (a [^y/_x]) =  \exists_w \exists_x(\delta_{x,y} \monop a) = 
%	\exists_x \exists_w(\delta_{x,y} \monop a) = 
%	\exists_x (\delta_{x,y} \monop \exists_w a) = 
%	(\exists_w a) [^y/_x]$.
%\end{proof}

Finally, we rephrase some further laws of the crisp case 
%It is easy to prove that the following laws hold 
(see~\cite[p.140]{pippo}).

\begin{lemma}
	\label{lemmaSubs}
	Let $\sigma \in F(V)$ be invertible. Then it holds
	\begin{enumerate}
		\item $\forall a\in A.\ (sv(\sigma) \setminus  \sigma(sv(\sigma))) \cap sv(s_\sigma a) = \emptyset$.
		\item $\forall a\in A.\ (\sigma(sv(\sigma)) \setminus sv(\sigma)) \cap sv(a) = \emptyset \implies s_{\sigma^{-1}} s_{\sigma} a = a$,
		%\item $s_{\sigma} a = a \iff sv(a) \cap sv(\sigma) = \emptyset$
	\end{enumerate}
\end{lemma}

\begin{proof}
	As for item $1$, note that $sv(s_{\sigma} a) = \sigma(sv(a))$. Let $Z = sv(\sigma) \cap sv(a)$: 
	then $\sigma(Z) \subseteq \sigma(sv(\sigma))$ and $\sigma \mid_{sv(a) \setminus Z} = \iota \mid_{sv(a) \setminus Z}$, which implies $(sv(a) \setminus Z) \cap sv(\sigma) = \emptyset$.
	Since $\sigma(Z) \subseteq \sigma(sv(\sigma))$, it holds $(sv(\sigma) \setminus \sigma(sv(\sigma)) \cap \sigma(sv(a)) = (sv(\sigma) \setminus \sigma(sv(\sigma)) \cap sv(s_{\sigma} a) = \emptyset$.

	As for item $2$, by definition it holds $s_{\sigma^{-1}} s_{\sigma} a = s_{\sigma^{-1} \circ \sigma} a$. 
	Now, item $3$ of Lemma~\ref{lemma:Inv0} implies that $sv(\sigma^{-1} \circ \sigma) = \sigma(sv(\sigma)) \setminus sv(\sigma)$, 
	and since $\sigma(sv(\sigma)) \cap sv(a) = \emptyset$ it holds $(\sigma^{-1} \circ \sigma) \mid _{sv(a)} = \iota \mid _{sv(a)}$.
	The result then follows by item $2$ of Definition~\ref{def:poly}.	
\qed	
	%Consider now item $3$: note that $a = b \implies \exists_X a = \exists_X b$. Then $s_\sigma a = a \implies \exists_{\sigma(sv(a))} s_\sigma a = \exists_{sv(a)} a \implies s_\sigma \exists_{\sigma^{-1}(sv(a))} a = \exists_{sv(a)} a \implies \sigma^{-1}(sv(a)) = sv(a) \implies sv(\sigma) \cap sv(a) = \emptyset$. \\
	%Questo pero' mi confonde: se $\sigma$ fosse una permutazione varrebbe lo stesso. \\
	%To proove $sv(\sigma) \cap sv(a) \implies s_{\sigma} a = a$, consider the contrapositive of item $2$ in Definition~\ref{def:poly}: then it holds $s_{\sigma} a \not = a \implies \sigma \mid_{sv(a)} \not = \iota \mid_{sv(a)}$, i.e. there exists $X \subseteq sv(a) \not = \emptyset$ such that $\sigma(X) \not = X$, which in turn implies $sv(\sigma) \cap sv(a) \not = \emptyset$.
	%$\hat{\tau} \circ \hat{\sigma} = \hat{\tau}$, then $s_{\hat{\tau} \circ \hat{\sigma}} a = s_{\hat{\tau}} \exists_Y a$. Since $Y = sv(\hat{\tau})$, the result follows from $1$ of Lemma~\ref{lemmaSubs0}.
	%$\{^x/_y\} \mid_{V \setminus\{y\}} = id \mid_{V \setminus\{y\}}$.
	%As for $3$,  note that $\sigma^{-1} (X) = \emptyset$, thus
	%$\exists_X s_{\sigma} a = s_{\sigma} a$ and the result holds.
%	Consider for instance the most difficult item $3$.
%	If $x=y$ the proof is over. Now, since $w \not \in \{x, y\}$ we have 
%	by definition that
%	$\delta_{x,y} = \exists_w(\delta_{x,w} \monop \delta_{w,y})$.
%	Again by definition $b [^y/_x] = \exists_x(\delta_{x,y} \monop b)$, so that
%	$\exists_w (a [^y/_x]) =  \exists_w \exists_x(\delta_{x,y} \monop a) = 
%	\exists_x \exists_w(\delta_{x,y} \monop a) = 
%	\exists_x (\delta_{x,y} \monop \exists_w a) = 
%	(\exists_w a) [^y/_x]$.
\end{proof}

\subsection{Cylindric and Polyadic Operators for Residuated Monoids}
\label{cyre}
Both algebraic structures introduced in the previous section are quite standard,
even if polyadic operators are less-known in the soft-constraints literature:
we tailored their presentation to our needs, and indeed the properties
presented in Section~\ref{propo} appear to be original. It is now time to consider 
the interaction of such structures with residuation. 
%
To this end, in the following we assume that 
$\mathbb{S}$ is a RePO (see Definition~\ref{def:repo}).


\begin{lemma}
Let $X \subseteq V$ be finite. Then it holds
	\begin{itemize}
         \item $\forall a, b \in A.\ \exists_x(a \odiv \exists_x b) \leq \exists_x a \odiv \exists_x b$.
	\end{itemize}
\end{lemma}

\begin{proof}
 \[\exists_x b \otimes (a \odiv \exists_x b) \leq a \implies
   \exists_x(\exists_x b \otimes (a \odiv \exists_x b)) \leq \exists_x a \implies\]
 \[\exists_x b \otimes \exists_x(a \odiv \exists_x b)) \leq \exists_x a \implies
   \exists_x(a \odiv \exists_x b) \leq \exists_x a \odiv \exists_x b\]
   \qed
\end{proof}

\begin{remark}
Looking at the proof above, it is clear that $\exists_x(a \odiv \exists_x b) \leq \exists_x a \odiv \exists_x b$
is actually equivalent to state that
$\exists_x(a \monop \exists_x b) \geq \exists_x a \monop \exists_x b$.
\end{remark}

%\begin{remark}
%\todo{un esempio dove $\odiv$ non distribuisce}
%\end{remark}

Similarly, it is easy to show that it holds $\forall a, b \in A.\ \exists_x(\exists_x a \odiv b) \leq \exists_x a \odiv \exists_x b$. 
%
A similar result relates residuation and polyadic operators.
%the following lemma holds.
%\todo{Mettere motivazione Lemma?}

\begin{lemma}
Let $\sigma \in F(V)$. Then it holds
\begin{itemize}
\item $\forall a,b \in A.\ s_\sigma (a \odiv b) \leq s_\sigma a \odiv s_\sigma b$.
\end{itemize}
\end{lemma}

\begin{proof}
\[ a \otimes (b \odiv a) \leq b \implies \]
\[ s_\sigma [a \otimes (b \odiv a)] \leq s_\sigma b \implies \]
\[ s_\sigma a \otimes s_\sigma(b \odiv a) \leq s_\sigma b \implies \]
\[ s_\sigma (b \odiv a) \leq s_\sigma b \odiv s_\sigma a \]
\end{proof}

Furhtermore, if $\sigma$ is invertible, then we can prove the following:

\begin{lemma}
Let $\sigma \in F(V)$ be invertible. Then it holds
\begin{itemize}
\item $\forall a,b \in A.\ s_\sigma (a \odiv b) = s_\sigma a \odiv s_\sigma b$.
\end{itemize}
\end{lemma}

\begin{proof}
First of all, note that $\sigma$ invertible implies $s_{\sigma^{-1}} s_{\sigma} a = a 
= s_{\sigma} s_{\sigma^{-1}} a$, from which we can prove:
\[ s_\sigma b \otimes (s_\sigma a \odiv s_\sigma b) \leq s_\sigma a  \implies \]
\[ s_{\sigma^{-1}} (s_\sigma b \otimes (s_\sigma a \odiv s_\sigma b))
\leq s_{\sigma^{-1}} s_\sigma a  \implies \]
\[ s_{\sigma^{-1}} s_\sigma b \otimes s_{\sigma^{-1}} (s_\sigma a \odiv s_\sigma b) \leq a \implies \]
\[ b \otimes s_{\sigma^{-1}} (s_\sigma a \odiv s_\sigma b) \leq a \implies \]
\[ s_{\sigma^{-1}} (s_\sigma a \odiv s_\sigma b) \leq a \odiv b \implies \]
\[ s_\sigma (s_{\sigma^{-1}} (s_\sigma a \odiv s_\sigma b)) \leq s_\sigma (a \odiv b) \implies \]
\[ s_\sigma a \odiv s_\sigma b \leq s_\sigma (a \odiv b) \]
\end{proof}

\section{Polyadic Soft Constraints}\label{sec:softconstraints}
\label{subsec:inst} 
In the past sections we mentioned a few ReSLs such as 
the Fuzzy 
%semiring  $\langle [0, 1], \leq, \times, 1 \rangle$ 
%of the $[0, 1]$ interval of real numbers with the usual order and multiplication
%as the monoidal operator
and the Tropical semiring.
%$\langle \mathbb{R}^+ \cup\{+\infty\}, \geq, +, 0 \rangle$
%non-negative reals plus $\infty$ with the inverse order and addition.
%
Building on such examples, in this section we give a main case study: a ReSL 
where the notion of cylindric and polyadic operators can be easily given.
It exploits the notion
of soft constraint: indeed, our proposal follows yet generalises \cite{scc},
whose underlying algebraic structure is the one of absorptive semirings.

\begin{definition}[(soft) constraints]\label{def:softconstraints}
	Let $V$ be a set of variables, $D$ a finite domain of interpretation
	and ${\mathbb S} = \langle A, \leq, \monop, \odiv, \monid \rangle$ a ReSL.
	A \emph{(soft) constraint} $c: (V \rightarrow D) \rightarrow
	A$ is a function associating a value in $A$ with each assignment
	$\eta: V\rightarrow D$ of the variables.
\end{definition}

In this section and in the following one, we denote by $\mathcal{C}$ the set of constraints that can be
built starting from chosen $\mathbb S$, $V$, and $D$. The application of a
constraint function $c:(V \rightarrow D) \rightarrow A$ to a variable
assignment $\eta:V\rightarrow D$ is denoted $c\eta$.  

Even if
a constraint involves all the variables in $V$, it may depend on
the assignment of a finite subset of them, called its support. For
instance, a binary constraint $c$ with $supp(c)=\{x,y\}$ is a function
$c: (V\rightarrow D)\rightarrow A$ that depends only on the
assignment of variables $\{x,y\}\subseteq V$, meaning that two
assignments $\eta_1, \eta_2: V \rightarrow D$ differing only for the
image of variables $z \not \in \{x,y\}$ coincide (i.e., $c\eta_1 =
c\eta_2$).
%
The support corresponds to the classical notion of scope of a
constraint.  We often refer to a constraint with support $X$ as $c_X$.
Moreover, an assignment over a support $X$ of cardinality $k$ is concisely
represented by a tuple $t$ in $D^k$, and we often write $c_X(t)$
instead of $c_X\eta$.

\smallskip
The set of constraints forms a ReSL, with the structure
lifted from ${\mathbb S}$.

\begin{lemma}[the ReSL of constraints]\label{prop:soft}
	The ReSL of constraints $\mathbb C$ is
	defined as the tuple $\langle {\mathcal C}, \leq, \monop, \odiv, \monid \rangle$ such that
	
	\begin{itemize}
		\item $c_1 \leq c_2$ if $c_1\eta\leq c_2\eta$ for all $\eta: V \rightarrow D$,
		\item $(c_1\monop c_2)\eta = c_1\eta\monop c_2\eta$, %for $c_1, c_2\ \in {\mathcal C}$
		\item $(c_1\odiv c_2)\eta = c_1\eta\odiv c_2\eta$, %for $c_1, c_2\ \in {\mathcal C}$
		\item $\monid \eta = \monid$.
	\end{itemize}
\end{lemma}


Combining constraints by the $\monop$ operator
means building a new constraint whose support involves at most
the variables of the original ones. The resulting constraint  associates with
each tuple of domain values for such variables the element
that is obtained by multiplying  those associated by the
original constraints to the appropriate sub-tuples.
%
%Residuation works as expected (i.e., $(c_1\odiv c_2)\eta = c_1\eta\odiv c_2\eta$),
%and 
%Also, the bottom is the constant function mapping all $\eta$ to $\bot$.

%\begin{example}[A simple CLIM]\label{execlim}
%Let us consider a CLIM $\mathbb S$, 
%and as $D$ a finite subset of the elements of the CLIM.
%A polynomial with variables in $V$ 
%and elements of the CLIM as coefficients
%such as $ux \, \hat{+} \, vy \, \hat{+} \, z$
%can be interpreted as the soft constraint associating 
%with a function $\eta: V \rightarrow D$ the value 
%$\bigvee \{u \monop \eta(x), v \monop \eta(y), z \}$.
%The composition of such constraints is straightforward, while 
%the ordering might not be the one induced by the coefficients, 
%due to the presence of constants.
%
%More precisely, let us consider the CLIM of non-negative reals and
%the polynomials $2x \, \hat{+} \, 1$ and $x \, \hat{+} \, 6$
%and let us assume $D = \{1, 2, 3\}$.
%The composition of such constraints is actually given just by coefficient 
%addition, so that
%$(2x \, \hat{+} \, 1) \monop (x \, \hat{+} \, 6) = 
%(3x \, \hat{+} \, 7)$.
%However, note that $2x \, \hat{+} \, 1 \leq x \, \hat{+} \, 6$.
%
%
%Similarly for residuation, which is just bounded subtraction of coefficients.
%Since $2x \, \hat{+} \, 1 \leq x \, \hat{+} \, 6$,
%by construction ($2x \, \hat{+} \, 1) \odiv (x \, \hat{+} \, 6)$ is the bottom constraint,
%mapping all variables to $0$.
%Instead, $(x \, \hat{+} \, 6) \odiv (2x \, \hat{+} 1)$ could be described as $\hat{-}x \, \hat{+} \, 5$,
%even if
%the latter falls outside of the polynomials we considered since it has a negative coefficient:
%it suffices to assume that if the actual result of the evaluation of the polynomial is negative 
%then it is put to $0$.
%
%%
%If $D$ is not the singleton, the support of a polynomial is precisely the set of variables occurring in it.
%\end{example}

%The ReSL of constraints also enjoys the cylindric properties, as shown by
%the result below (for cylindric operators and diagonals in the idempotent case, see~\cite{scc}).

\begin{lemma}[Cylindric and polyadic operators for (soft) constraints]
	The ReSL of constraints $\mathbb{C}$ admits cylindric and polyadic operators, defined as
	\begin{itemize}
		\item  $(\exists_x c) \eta = \bigvee \{c \rho \mid \eta\mid_{V \setminus \{x\}} = 
		\rho\mid_{V \setminus \{x\}}\}$ for all $c \in {\mathcal C}, x \in V$
		%\item if $\sigma$ is an injective substitution, then $(s_{\sigma}c)\eta = c(\sigma \circ \eta)$ 
		%for all $c \in \mathcal{C}$
		\item  $(s_\sigma c) \eta = c (\eta \circ \sigma)$ for all $c \in {\mathcal C}, \sigma \in F(V)$	
%		\item $\delta_{x,y}\eta = \left\{
%		\begin{array}{rcl} \bot & & \text{if } \eta(x) = \eta(y); \\
%		\top & & \text{otherwise.}
%		\end{array} \right.$ for all $x, y \in V$
	\end{itemize}
\end{lemma}

Hiding means eliminating variables from the support:
$supp(\exists_x c) \subseteq supp({c}) \setminus {x}$.\footnote{The operator
	is called \emph{projection} in the soft framework,
	and $\exists_x c$ is denoted $c\Downarrow_{V\setminus \{x\}}$.}

\begin{proof}
%Let us consider a finite set $X$ and a constraint $c$. Recall that since $supp(c)$ is finite,
%$c$ can be considered as a function $D^k \rightarrow A$ for $k = \# supp(c)$.
%
%Thus $(\exists_X c) \eta$ just boils down to consider the LUB of $\# supp(c) \setminus X$ 
%sets of $k$-tuples, and t
The properties of the pomonoid action (see Definition~\ref{pomo})
are easily shown to hold for both operators. 
%
As for the cylindric laws (see Definition~\ref{cyli}), first note that the set of functions 
$\rho$ such that $\eta\mid_{V \setminus \{x\}} = \rho\mid_{V \setminus \{x\}}$ is actually finite.
Thus, we have that 
\begin{eqnarray*}
(\exists_x (c \otimes \exists_x d)) \eta & = & \bigvee_\rho \{(c \otimes \exists_x d) \rho \mid \eta\mid_{V \setminus \{x\}} = \rho\mid_{V \setminus \{x\}}\}\\
& = & \bigvee_\rho \{c\rho \otimes (\exists_x d) \rho \mid \eta\mid_{V \setminus \{x\}} = \rho\mid_{V \setminus \{x\}}\} \\
& = & \bigvee_\rho \{c\rho \otimes  (\bigvee_\xi \{d \xi \mid \rho\mid_{V \setminus \{x\}} = \xi\mid_{V \setminus \{x\}}\}) \mid \eta\mid_{V \setminus \{x\}} = \rho\mid_{V \setminus \{x\}}\} \\
& = & \bigvee_\rho \{c\rho \mid \eta\mid_{V \setminus \{x\}} = \rho\mid_{V \setminus \{x\}}\} \otimes \bigvee_\xi \{d \xi \mid \eta\mid_{V \setminus \{x\}} = \xi\mid_{V \setminus \{x\}}\} \\
& = & (\exists_x c) \eta \otimes (\exists_x d) \eta
\end{eqnarray*}

% = 
% =\]
%= \]
% =\]
% =
%(\exists_X c) \eta \otimes (\exists_X d) \eta \]
%%\[ (\exists_X d) \rho = \bigvee \{d \xi \mid \rho\mid_{V \setminus X} = \xi\mid_{V \setminus X}\}\]

[da rivedere]
Let us now move to the polyadic laws (see Definition~\ref{def:poly}).We just consider the third item, 
and we assume that $\sigma \mid_{\sigma^c(X)}$ is injective, thus
\begin{eqnarray*}
(\exists_X s_\sigma c) \eta & = &\bigvee_\rho \{(s_\sigma c) \rho \mid \eta\mid_{V \setminus X} = \rho\mid_{V \setminus X}\} = \bigvee_\rho \{c (\rho \circ \sigma) \mid \eta\mid_{V \setminus X} =   \rho\mid_{V \setminus X}\} \\ 
& = & \bigvee_\xi \{c\xi \mid (\eta \circ \sigma)\mid_{V \setminus \sigma^{c}(X)}  =  \xi\mid_{V \setminus \sigma^{c}(X)}\} \\
& = & (\exists_{\sigma^c(X)} c) (\eta \circ \sigma) = (s_\sigma \exists_{\sigma^c(X)} c) \eta
\end{eqnarray*}


%  \bigvee \{c (\rho \circ \sigma) \mid (\eta\circ \sigma)\mid_{V \setminus \sigma^{c}(X)} = (\rho\circ \sigma)\mid_{V \setminus \sigma^{c}(X)}\}=^{*}\]
\noindent
where it always holds that $\eta\mid_{V \setminus X} = \rho\mid_{V \setminus X}$ implies $(\eta\circ \sigma)\mid_{V \setminus \sigma^{c}(X)} = (\rho\circ \sigma)\mid_{V \setminus \sigma^{c}(X)}$,
while since $\sigma \mid_{\sigma^c(X)}$ is injective we have that
a $\xi$ satisfying $(\eta \circ \sigma)\mid_{V \setminus \sigma^{c}(X)} = \xi\mid_{V \setminus \sigma^{c}(X)}$ can be decomposed as $\rho\circ \sigma$
for a $\rho$ such that $\eta\mid_{V \setminus X} = \rho\mid_{V \setminus X}$
(otherwise, it could happen that for some $\{x, y\} \subseteq \sigma^c(X)$ we have that $\sigma(x) =\sigma(y)$ and 
$\xi(x) \neq \xi(y)$).
\qed
\end{proof}

%
%Finally, the diagonal element $\delta_{x,y}$ has support $\{x, y\}$ for 
%$x \neq y$, while the support for $\delta_{x,x}$ is $\emptyset$. 


\comment{
Note also that the diagonal elements are not guaranteed to be $\monop$-compact,
even if they have finite support, since $\top$ is not necessarily so.
%
To this end, we close the section by adding the simple result below to the soft constraint lore.

\begin{proposition}
	Let $c \in \mathbb{C}$ be a constraint. It is $\monop$-compact if and only if it has finite support and 
	$c\eta$ is $\monop$-compact for all $\eta$.
\end{proposition}
}

\section{Deterministic Soft CCP}\label{sec:detSCCP}
This section introduces our (meta-)language.
We fix a set of variables $V$, ranged over by $x$, $y$, $\ldots$ , and 
an invertible CLIM $\mathbb S = \langle {\mathcal C}, \leq, \otimes\rangle$, which is 
cylindric over $V$ and whose compact elements
are ranged over by $c$, $d$, $\ldots$.

\begin{definition}[Agents]%
The set $\mathcal{A}$ of all agents, %which is
parametric with respect to a set $\mathcal{P}$ of (unary) procedure declarations $p(x)$,
is given by the following grammar
\[ A \Coloneqq \: \: \mathit{\ostop} \mid \textit{\tell}(c)  \mid \textit{\ask}(c) \rightarrow A \mid A \parallel A \mid %\exists_x A \mid %Z \mid \mu_Z A 
p(x) \mid \exists^{\rho}_x A.\]  
\end{definition}

We consider here the extended agent $\exists^{\rho}_x A$, where $\rho$ is meant to represent a local store. More precisely, the extended agent allows to carry some information about the hidden variable $x$ in an incremental way. In the following, we will often write $\exists_x A$ for $\exists^{\monid}_x A$ \\
We denote by $fv(A)$ the set of free variables of an agent, defined in the expected way 
by structural induction, assuming that $fv(\tell(c)) = sv(c)$ and
$fv(\ask(c) \rightarrow A) = sv(c) \cup fv(A)$. We also remark that $fv(\exists_x A) = fv(A) \setminus 
\{x\}$ and $fv(\exists^{\rho}_x A) = (fv(A) \setminus \{x\}) \cup sv(\rho)$.
%
In the following, we restrict our attention to 
procedure declarations $p(x) = A$ such that $fv(A) = \{x\}$.

We now move to consider the reduction semantics of our language.

\begin{definition}[Substitutions]
Let $[^y/_x] \in F(V)$, defined as: 
\[ [^y/_x](w) = 
		\begin{cases} 
			y & \text{if $w = x$} \\
            w & \text{otherwise}
        \end{cases} \].
We define the substitution operator $[^y/_x]: \mathcal{A} \rarrow \mathcal{A}$ on agents as follows: 

\begin{itemize}
	\item $[^y/_x] \ostop = \ostop$
	\item $[^y/_x] \tell(c) = \tell(s_{[^y/_x]}c)$
	\item $[^y/_x] p(w) =  p([^y/_x](w))$
	\item $[^y/_x] (\ask(c) \rightarrow A) = \ask(s_{[^y/_x]}(c)) \rightarrow [^y/_x] A$
	%\item $[^y/_x] (\exists_w A)  = \exists_w ([^y/_x] A) \ \ \text{for $w \not \in \{x, y\}$}$
    \item $[^y/_x] (\exists^{\rho}_w A) = \exists^{(s_{[^y/_x]} \rho)}_w [^y/_x]A \ \
    \text{for $w \not \in \{x, y\}$}$
	\item $[^y/_x](A_1 \parallel A_2)  = ([^y/_x] A_1 \parallel [^y/_x] A_2)$
\end{itemize}
\end{definition}

\begin{remark}
In the following, we consider terms to be equivalent up to $\alpha$-conversion, meaning that terms which differ only for hidden variables can be considered equivalent:
\[\exists_x^\rho A = \exists_y^{(s_{[^y/_x]}\rho)} A [^y/_x] \ \ for \ \ y \not \in sv(\rho) \cup fv(A)\].
Note that this holds thanks to the underlying monoid properties, where we have that $\exists_x \sigma = \exists_y s_{[^y/_x]}(\sigma)$ if $y \not \in sv(\sigma)$.
\end{remark}

Thanks to $\alpha$-conversion, we can define substitution for $\exists^{\rho}_x A$ as above, even if the substitution function $[^y/_x]$ is not injective. Other cases can be managed by renaming, since the following holds:

\begin{proposition}
Let $A \in \mathcal{A}$, $x \not \in fv(A)$. Then $[^y/_x]A = A$.
\end{proposition}

\begin{proof}
For $\ostop$, $\tell(c)$, $p(w)$ the statement is trivially true, as we have that
\begin{itemize}
	\item $[^y/_x]\ostop = \ostop$
	\item $[^y/_x]\tell(c) = \tell(s_{[^y/_x]}c) = \tell(c)$ by polyadic laws and $x \not
	\in fv(\tell(c)) = sv(c)$.
	\item $[^y/_x]p(w) = p(w)$ by $x \not = w$.
\end{itemize}
As for the other agents, the statement can be proved by inductive reasoning, assuming it to be true for subagents in $\ask(c) \rightarrow A$, $(A_1 || A_2)$ and $\exists^{\rho}_w A$. Indeed we obtain:
\begin{itemize}
	\item $[^y/_x](\ask(c) \rightarrow A) = [^y/_x]\ask(c) \rightarrow [^y/_x]A = 
	\ask(s_{[^y/_x]}c) \rightarrow [^y/_x]A = \ask(c) \rightarrow A$
	by hypothesis and polyadic laws.
	\item $[^y/_x](A_1 || A_2) = ([^y/_x]A_1 || [^y/_x]A_2)) = (A_1 || A_2)$
	by hypothesis.
	\item $[^y/_x](\exists^{\rho}_w A) = \exists^{s_{[^y/_x]\rho}}_w [^y/_x]A = \exists^{\rho}_w A$
	by hypothesis and polyadic laws.
\end{itemize}
\end{proof}

\begin{definition}[Reductions]\label{def:reductions}
Let $\Gamma = {\mathcal A} \times \C$ be the set of \emph{configurations}.
The \emph{direct reduction semantics} for SCCP is the pair 
$\langle \Gamma,  \mapsto \rangle$
such that $\mapsto \, \, \subseteq \, \,\Gamma \times   \Gamma$ is the family 
 of binary relations indexed over sets of variables,
%$2^V$,
%between them, 
i.e., $\mapsto = \bigcup_{\Delta \subseteq V} \mapsto_\Delta$ and 
$\mapsto_\Delta \, \, \subseteq \, \,\Gamma \times \Gamma$, obtained by the rules in 
Table~\ref{fig:operational}.

The \emph{reduction semantics} for SCCP is the pair 
$\langle \Gamma,  \rightarrow \rangle$
such that $\rightarrow \, \, \subseteq \, \,\Gamma \times   \Gamma$ is the family 
 of binary relations indexed over sets of variables,
%$2^V$,
%between them, 
i.e., $\rightarrow = \bigcup_{\Delta \subseteq V} \rarrow_\Delta$ and 
$\rarrow_\Delta \, \, \subseteq \, \,\Gamma \times \Gamma$, obtained by the rules in 
Table~\ref{fig:operational} and Table~\ref{fig:operational2}.
\end{definition}

%\vspace{-.25cm}
\def\odiv{\; {\ominus\hspace{-6pt} \div} \;}
\def\odivvv{\; {\ominus\hspace{-6pt} \div} \;}

\begin{table}  %\hfil5
  %\scalebox{0.9}{
   \begin{center}
   \begin{tabular}{llll} 
   %
   \mbox{\bf A1}& $\frac{\displaystyle sv(\sigma) \cup sv(c) \subseteq \Delta } {\displaystyle \langle \hbox{\tell}(c), \sigma \rangle \mapsto_\Delta  \langle \hbox{\ostop},
                                               \sigma \otimes c\rangle}$
   \ \ \ & \bf{Tell}&
  \\ 
  &\mbox{   }&\mbox{   } &\mbox{   }
  \\
  \mbox{\bf A2}& $\frac {\displaystyle sv(\sigma) \cup sv(c) \cup fv(A) \subseteq \Delta \wedge c \leq \sigma}{\displaystyle
  	\begin{array}{l} \langle \hbox{\ask}(c) \rightarrow A, \sigma \rangle \mapsto_\Delta \langle A, \sigma \rangle   	\end{array}}$
    \ \ \ & \bf{Ask}&
    \\
    &\mbox{   }&\mbox{   }&
    \\
  \mbox{\bf A3}& $\frac {\displaystyle sv(\sigma) \cup \{y\} \subseteq \Delta \wedge \displaystyle p(x) = A \in \mathcal{P} }
  {\displaystyle\langle p(y),\sigma\rangle \mapsto_\Delta \langle [^y/_x]A, \sigma \rangle}$ 
  &\bf{Rec}&
    %\\
    %&\mbox{   }&\mbox{   }&
    %\\
    %\mbox{\bf A4}& $\frac {\displaystyle sv(\sigma) \cup fv(\exists_x A) \subseteq 
    %\Delta \wedge w \not \in \Delta }
    %{\displaystyle\langle \exists_x A,\sigma\rangle \mapsto_\Delta \langle [^w/_x]A,
    %\sigma\rangle}$
    %&\bf{Hide}&
    %\\
   %&\mbox{   }&\mbox{   }&
  \\
    \mbox{\bf A4}& $\frac {\displaystyle \langle A, \rho \otimes \sigma_0 \rangle
    \mapsto \langle B, \sigma_1 \rangle \wedge x \not \in sv(\sigma) \wedge \sigma_0 
    = \sigma \odiv \exists_x \rho }
    {\displaystyle\langle \exists^{\rho}_x A,\sigma\rangle \mapsto_\Delta \langle 
    \exists^{\sigma_1 \odiv \sigma_0}_x B, \sigma_0 \otimes \exists_x(\sigma_1 \odiv \sigma_0) 
    \rangle}$
    &\bf{Hide-local}&
  \end{tabular}
  \end{center}
\caption{Axioms of the reduction semantics for SCCP.}
\label{fig:operational}
\end{table}

\begin{table}  %\hfil5
  %\scalebox{0.9}{
   \begin{center}
   \begin{tabular}{llll} 
   %
  \mbox{\bf R1}& $\frac {\displaystyle \langle A,\sigma \rangle \rightarrow_\Delta \langle A', \sigma' \rangle
  \wedge fv(B) \subseteq \Delta} 
  {\displaystyle \begin{array}{l}
                          \langle A\parallel B, \sigma \rangle \rightarrow_\Delta \langle A'\parallel B, \sigma' \rangle
                          \end{array}}$ 
    & \bf{Par1}&
  \\
  & \mbox{   }&\mbox{   }&
  \\
    \mbox{\bf R2}& $\frac {\displaystyle \langle A,\sigma \rangle \rightarrow_\Delta \langle A', \sigma'   \rangle
    	\wedge fv(B) \subseteq \Delta} 
    {\displaystyle 
    	\begin{array}{l} \langle B\parallel A, \sigma \rangle \rightarrow_\Delta \langle B\parallel A', \sigma' \rangle
    	\end{array}}$& \bf{Par2}&
  \end{tabular}
  \end{center}
\caption{Contextual rules of the reduction semantics for SCCP.}
\label{fig:operational2}
\end{table}

The split distinguishes between axioms and rules guaranteeing the closure with respect to the parallel operator. Indeed, rules {\bf  R1}  and {\bf  R2} model the interleaving of two agents in parallel.
%
%
In {\bf A1} a constraint $c$ is added to the store $\sigma$.
%, which in the next step will be $\sigma \otimes c$.
%
{\bf A2} checks if $c$ is entailed by  $\sigma$: if not, the computation is blocked.

Axiom {\bf A3} replaces a procedure identifier with the associated body, renaming the formal parameter with the actual one:
%$A[^y/_x]$ stands for the agent obtained by replacing all the occurrences of $x$ with $y$.
%
%Axiom {\bf A4} hides the variable $x$ occurring in $A$, replacing it  
%with a globally fresh variable,
%as ensured by $w \not \in \Delta$.
%The latter is more general than just requiring that 
%$w \not \in fv(\exists_x A) \cup sv(\sigma)$, since
%$\langle B, \rho \rangle   \rarrow_\Delta$ implies that 
%$fv(B) \cup sv(\rho) \subseteq \Delta$.\footnote{Our rule is  reminiscent of 
%$(8)$ in~\cite[p.~342]{popl91}.}

In axiom {\bf A4} we consider instead local variables, where the hiding operator carries some information on the variables it abstracts.
More precisely, according to~\cite{extendedHiding} we consider an extended operator $\exists_x^\rho$, for $\rho$ the local store.
Thanks again to the residuation operator, the rule for the extended hidings can be defined as {\bf Hide-local}. The precondition states that $x \not \in sv(\sigma)$: this ensures that $\sigma_0$ does not contain free occurences of $x$, as it is obtained by removing $\exists_x \rho$ from $\sigma$. The intuition is then that $x$ can only appear in the local store $\rho$, and then in $\sigma_1$. Thus, hiding $x$ means to remove it from the resulting store, by letting it $\sigma_0 \otimes \exists_x(\sigma_1 \odiv \sigma_0)$. On the other hand, the local information carried by the hide operator is updated to $\sigma_1 \odiv sigma_0$, which can still contain $x$. \\
\smallskip

Let $\gamma = \langle A, \sigma \rangle$ be a configuration.
%
We denote by $fv(\gamma)$ the set $fv(A) \cup sv(\sigma)$ and by
$\gamma[^z/_w]$ the component-wise application of substitution $[^z/_w]$.

\begin{lemma}[On monotonicity]
\label{mono}
Let $\langle A, \sigma \rangle \rightarrow_\Delta \langle B, \sigma' \rangle$ be a reduction. 
Then
\begin{enumerate}
\item $sv(\sigma') \subseteq sv(\sigma)\cup fv(A) \subseteq \Delta$;
\item $\sigma \leq \sigma'$;
\item $\langle A, \sigma \rangle \rightarrow_{\Delta'} \langle B, \sigma' \rangle$
         for all $\Delta'. sv(\sigma)\cup fv(A) \subseteq \Delta' \wedge fv(B) \cap \Delta' \subseteq fv(A)$;
\item $\langle A, \sigma \otimes \rho \rangle \rightarrow_\Delta \langle B, \sigma' \otimes \rho \rangle$
         for all $\rho \in  \C^\otimes. sv(\rho) \subseteq \Delta$. 
\end{enumerate}
 \end{lemma}
 
 All statements are straightforward. 

As for item 1, by construction $sv(\sigma)\cup fv(A) \subseteq \Delta$: since 
only rule {\bf A1} can modify the store
and $sv(\sigma \otimes c) \subseteq sv(\sigma) \cup sv(c)$, then the statement holds.
Similarly for item 2, since $\sigma \leq \sigma \otimes c$.
Item 3 is again true by construction. 
As for item 4, it suffices to also note that 
$\sigma, \rho \in  \C^\otimes$ ensure that $\sigma \otimes \rho \in  \C^\otimes$
and clearly {\bf A2} will still be executable. 

\begin{definition}[Increasing computations]\label{def:min}
Let $\gamma_0  \rightarrow_{\Delta_1} \gamma_1  \rightarrow_{\Delta_2} \gamma_2 \rightarrow_{\Delta_3} \dots$ be a
(possibly infinite) computation. 
%\marginpar{added ``infinite'' before computation}
It is increasing if $\Delta_k \subseteq \Delta_{k+1}$ for any $k >1$, and
it is minimally increasing if $\Delta_k + fn(\gamma_k) = \Delta_{k+1}$ for any $k>1$.
\end{definition}

What is noteworthy is that in such computation the sets $\Delta$'s are always uniquely identified,
once $\Delta_1$ is fixed.
Thanks to Lemma \ref{mono}, for the sake of simplicity and without loss of generality 
in the following we restrict our attention to minimally increasing computations, dropping
altogether the subscripts $\Delta$'s whenever they are irrelevant.

\subsection{Observables and local confluence}
The direct reduction semantics is at its heart deterministic, even of there can be some issues concerning the choice of the fresh variable introduced.

\begin{lemma}\label{lemma:uptoD}
Let $\gamma \mapsto_\Delta \gamma_i$ be direct reductions for $i =  1, 2$.
%such that $\gamma_1 \cong_\Delta \gamma_2$ and $\Delta \subseteq \Delta_1 \cup \Delta_2$.
Then either $\gamma_1 = \gamma_2$ or
\begin{itemize}
\item $\gamma \mapsto_\Delta \gamma'$ with 
$\gamma' = \gamma_1[^z/_{w_1}] =  \gamma_2[^z/_{w_2}]$
for $w_i \in fv(\gamma_i) \setminus \Delta$ and $z$ fresh.
\end{itemize}
\end{lemma}

\begin{proof}
The direct reduction semantics is deterministic, and the only freedom is the choice 
of the fresh variable in rule {\bf A4}. 
Let us assume that  different variables are chosen, let us say $w_1$ and $w_2$.
However, $\gamma_1[^z/_{w_1}] =  \gamma_2[^z/_{w_2}]$ by replacing the new variables 
with a globally fresh one, and the existence of $\xi$ is immediate.
\end{proof}

In order to tackle the whole reduction semantics, we need to define a suitable notion of observable for a computation.

\begin{definition}[Observables]\label{def:observables}
Let $\xi = \gamma_0  \rightarrow \gamma_1  \rightarrow \dots$ be a (possibly infinite) computation with $\gamma_i = \langle A_i, \sigma_i\rangle$.
%
Then its observation $\mathit{Result}(\xi)$ 
is $\bigvee_i (\exists_{X_i} \sigma_i)$, for $X_i = (fv(\gamma_i))\setminus(fv(\gamma_0))$.
%$\bigvee_{i} \sigma_i$.
\end{definition}

For finite computations the result is the store of the last configuration.

\begin{proposition}\label{lemma:upto}
Let $\gamma \rightarrow_\Delta \gamma_i$ be reductions for $i =  1, 2$.
%such that $\gamma_1 \cong_\Delta \gamma_2$ and $\Delta \subseteq \Delta_1 \cup \Delta_2$.
Then either $\gamma_1 = \gamma_2$ or 
\begin{enumerate}
\item $\gamma \rightarrow_\Delta  \gamma'$ with 
$\gamma' = \gamma_1[^z/_{w_1}] =  \gamma_2[^z/_{w_2}]$
for $w_i \in fv(\gamma_i) \setminus \Delta$ and $z$ fresh;
\item
$\xi_i = \gamma \rightarrow_\Delta \gamma_i \rightarrow_{\Delta_i} \gamma_3$
\item
$\xi_i = \gamma \rightarrow_\Delta \gamma_i[^{z_i}/_w] \rightarrow_{\Delta \cup \{z_i\}} \gamma_3$
for $w \in fv(\gamma_i) \setminus (\Delta \cup fv(\gamma_3))$ and $z_i$'s fresh.
\end{enumerate}
In the two latter cases, we have $Result(\xi_1) = Result(\xi_2)$.
\end{proposition}

\begin{proof}
	The first item is clearly due to the choice of different free variables 
	for the same hiding operator, as for direct reduction semantics.
	
	So, let us assume that the two reductions occur on the opposite sides of a parallel operator.
	A first relevant case is if both reductions replace an hiding operator with the same fresh variable
	$w$. However, it suffices to replace $w$ 
	with fresh variables $z_1$ and $z_2$ in  the two reductions, in order for item 3 to be verified.
	%
		
	Among the remaining cases, the only relevant one is if both actions add 
	different constraints to the store.
	%\medskip
	%
	So, let us assume that $\gamma = \langle A_1 \parallel A_2, \sigma \rangle$ such that 
	$\langle A_1, \sigma \rangle \rightarrow_\Delta \langle B_1, \sigma_1 \rangle$
	and
	$\langle A_2, \sigma \rangle \rightarrow_\Delta \langle B_2, \sigma_2 \rangle$.
	%Also, we may safely assume up-to that any locally fresh variable occurring in either $B_1$ or $B_2$ 
	%is globally fresh.
	%
	Note that since reduction semantics is monotone (Lemma~\ref{mono})
	and $\sigma$ is $\otimes$-compact, also $\sigma_1$
	is $\otimes$-compact and furthermore we have
	$\sigma_1 = \sigma \otimes (\sigma_1 \odiv \sigma)$.
	% (by Lemma~\ref{mono}).
	%
	Now,  monotonicity (again Lemma~\ref{mono}) ensures us that
	$\langle B_1 \parallel A_2, \sigma \otimes (\sigma_1 \odiv \sigma) \rangle
	\rightarrow_\Delta
	\langle B_1 \parallel B_2, \sigma \otimes (\sigma_1 \odiv \sigma) \otimes (\sigma_2 \odiv \sigma) \rangle$ 
	and by symmetric reasoning the latter configuration 
	is the one we were looking for.
\end{proof}


The result above is a local confluence theorem, which is expected, since the calculus is essentially deterministic.
The complex formulation is  due to the occurrence of hiding operators: as an example, different fresh variables may be chosen
for replacing $\exists_x$, such as  $w_1$ and $w_2$ in the first item above, and then a globally fresh variable $z$ has to 
be found for replacing them.

We close with a technical lemma concerning the substitution of fresh variables in a computation, which ensures that it is essentially captured by a substitution in the initial configuration.

\begin{lemma}\label{lem:SubComp}
Let $\xi = \gamma_0  \rightarrow \gamma_1  \rightarrow \dots$ be a (possibly infinite)
computation and $w$, $z$ variables such that $w \in fv(\gamma_0)$ and $z$ is fresh.
Then there exists a computation 
$\xi [^z/_w] = \gamma_0 [^z/_w]   \rightarrow \gamma_1 [^z/_w]  \rightarrow \dots$ 
such that $\mathit{Result}(\xi) [^z/_w] = \mathit{Result}(\xi [^z/_w])$.
\end{lemma}

\subsection{Fairness and observational equivalence}
In order to define fair computations, we introduce the notion of path.

\begin{definition}[Paths]
Let $A  \in \mathcal{A}$ be an agent. The set of paths of $A$ is the set of strings 
$P(A) \subseteq {\{0, 1\}^*}$ defined by structural induction as 
\begin{itemize}
\item $P(\0) = P(\tell(c)) = P(p(x)) = \{\epsilon\}$
\item $P(\ask(c) \rightarrow A) = P(\exists_x A) = \{\epsilon\} \cup  \{0\} \cdot P(A)$
\item $P(A_1 \parallel A_2) = \{\epsilon\} \cup  \{0\} \cdot P(A_1) \cup  \{1\} \cdot P(A_2)$
\end{itemize}
\end{definition}

Thus, given an agent $A$ and a path  $\pi \in \{0, 1\}^*$, the expression $A|_\pi$ denotes uniquely (at most) one sub-agent of $A$.

The next step is to introduce enabled and active paths.

\begin{definition}[Enabled/Active paths]
Let $\gamma = \langle A, \sigma \rangle$ be a configuration and $\pi$ a path is $A$. 
%
We say that $\pi$ is enabled in $\gamma$ if 
$\langle A|_\pi, \sigma \rangle \Rightarrow$ and
for all prefixes $\pi'$ of $\pi$ the sub-agent $A|_{\pi'}$ has the shape
$A_1 \parallel A_2$.

Let $\xi = \gamma \rightarrow \langle B, \rho \rangle$ be a reduction. 
We say that $\pi$ is active in $\xi$ if it is enabled in $\gamma$
and  $\langle A|_\pi, \sigma \rangle  \Rightarrow \langle B|_\pi, \rho \rangle$.
\end{definition}


The intuition is that any reduction is generated by an agent of the shape $\mathit{\tell}(c)$ or  $\mathit{\ask}(c) \rightarrow A$ 
or $p(x)$ or $\exists_x A$ via the application of precisely one instance of one of the axioms
of Table~\ref{fig:operational}.
%
And an agent of such  shape is \emph{active} in a reduction $\gamma \rightarrow \gamma'$ if 
it precisely generates that transition.

\begin{definition}[Fair computations]\label{def:fair}
Let $\gamma_0  \rightarrow \gamma_1  \rightarrow \gamma_2 \rightarrow \dots$ be a
(possibly infinite) computation. 
We say that fair if whenever a path
$\pi$ is enabled in some $\gamma_i$ then it is active in  
$\gamma_j  \rightarrow \gamma_{j+1}$ 
for some $j \geq i$.
\end{definition}

The definition is intuitively the correct one thanks to the lemma below.

\begin{lemma}
\label{lem:enab}
Let $\xi = \langle A, \sigma \rangle \rightarrow \langle B, \rho \rangle$ be a reduction
and $\pi$ a path is $A$. 
If $\pi$ is enabled in $\langle A, \sigma \rangle$ and not active in $\xi$ then it is enabled in 
$\langle B, \rho \rangle$ and $A\mid_{\pi} = B\mid_{\pi}$.
\end{lemma}

The proof is straightforward. Thus, 
similarly to crisp programming~\cite{popl91}, we conclude that 
if a finite computation is fair
then it is deadlocked and its result is the store of the last configuration.

\begin{theorem}[Confluence]\label{prop:confluence}
Let $\gamma$ be a configuration and $\xi_1$, $\xi_2$ two (possibly infinite)
computations of $\gamma$.
%
If $\xi_1$ and $\xi_2$ are fair, then $\mathit{Result}(\xi_1) = \mathit{Result}(\xi_2)$.
\end{theorem}

\begin{proof}
The result is a combination of Proposition \ref{lemma:upto} and Lemmata \ref{lem:SubComp}
and \ref{lem:enab}. So, let us assume to have two (possibly infinite) fair computations originating from $\gamma$, and let us consider their initial reductions $\gamma \rightarrow_\Delta \gamma_i$ 
for $i =  1, 2$. 
First thing, note that assuming the same subscript $\Delta$ is not restrictive since we work with minimally increasing reductions.
%
Now, according to Proposition \ref{lemma:upto}, we have two cases, as depicted on Figure~\ref{fig:conf1} and Figure~\ref{fig:conf2}.
\end{proof}

% QUESTA e' commentata, ma e' la prima figura che mi hai mandato
%	\begin{figure}[t]
%		\centering\scalebox{0.8}{
%			\begin{tikzpicture}
%			\tikzstyle{all nodes}=[inner sep=0pt,minimum size=0.2cm,circle,fill=black]
%			\draw 
%			node[circle,fill=black,scale=0.5](A)at(5.5,1){}
%			node(B)at(4,-1){}              
%			node(C)at(7,-1){}
%			node(D)at(2.5,-3){}
%			node(E)at(8.5,-3){}
%			node(F)at(1.75,-4){}
%			node(G)at(9.25,-4){};	
%					
%			\draw[->](A)--(C) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {$\Delta_1$};
%			\draw[->](A)--(B) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {$\Delta_1$};	
%			\draw[->](B)--(D) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {$\Delta_2'$};		
%			\draw[->](C)--(E) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {$\Delta_2$};	
%			\draw[dashed,-](B)--(F) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
%			\draw[dashed,-](C)--(G) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};	
%			\end{tikzpicture}}
%		\caption{Prova}
%		\label{fig:fig1}
%	\end{figure}
	
	
	
	\begin{figure}[t]
		\newcommand{\HEIGHT} {135pt}
		\hfil
		\begin{minipage}{.45\columnwidth}
			\vbox to \HEIGHT{
				\vfil
				\centering
				\begin{tikzpicture}[scale=0.8, transform shape]
				\tikzstyle{all nodes}=[inner sep=0pt,minimum size=0.2cm,circle,fill=black]
				\draw 
				node[circle,fill=black,scale=0.5](A)at(5.5,1){}
				node(B)at(4,-1){$\gamma_1$}              
				node(C)at(7,-1){$\gamma_2$} 
				node(D)at(5.5,-2){$\gamma_i[^z/_{z_i}]$} 
				node(E)at(4.75,-3){}
				node(F)at(3.25,-2){}
				node(G)at(7.75,-2){}
				node(H)at(6.25,-3){};	
				
				\draw[->](A)--(C) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {};%{$\Delta_1$};
				\draw[->](A)--(B) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};%$\Delta_1$};
				\draw[->](A)--(D) node[pos=.7, fill=white, opacity=0.2, text opacity=1,left] {};%{$\Delta_1$};	
				\draw[dashed,-](D)--(E) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
				\draw[dashed,-](B)--(F) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
				\draw[dashed,-](C)--(G) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
				\draw[dashed,-](D)--(H) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
				\end{tikzpicture}
				\vfil
			}
			\caption{Proposition~\ref{lemma:upto} (item 1) and Lemma~\ref{mono} ensure that we may collapse the initial reductions (thus confluence).}
			\label{fig:conf1}
		\end{minipage}
		\hfil
		\begin{minipage}{.45\columnwidth}
			\vbox to \HEIGHT{
				\vfil
				\centering
				\begin{tikzpicture}[scale=0.8, transform shape]
					\tikzstyle{all nodes}=[inner sep=0pt,minimum size=0.2cm,circle,fill=black]
					\draw 
					node[circle,fill=black,scale=0.5](A)at(5.5,1){}
					node(B)at(3.5,-0.5){$\gamma_1$}              
					node(C)at(7.5,-0.5){$\gamma_2$} 
					node(D)at(8.5,-1.25){}
					node(E)at(2.5,-1.25){}
					node(F)at(4,-2){$\gamma_1[^{z_1}/_w]$}
					node(G)at(7,-2){$\gamma_2[^{z_2}/_w]$}
					node(H)at(5.5,-5){$\gamma_3$}
					node(I)at(3.5,-3){}
					node(L)at(7.5,-3){};	
					
					\draw[->](A)--(C) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {};%{$\Delta_1$};
					\draw[->](A)--(B) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};%{$\Delta_1$};	
					\draw[dashed,-](B)--(E) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
					\draw[dashed,-](C)--(D) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
					\draw[->](A)--(F) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {};
					\draw[->](A)--(G) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {};
					\draw[->](F)--(H) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {};
					\draw[->](G)--(H) node[pos=.4, fill=white, opacity=0.2, text opacity=1,right] {};
					\draw[dashed,-](F)--(I) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
					\draw[dashed,-](G)--(L) node[pos=.4, fill=white, opacity=0.2, text opacity=1,left] {};
				\end{tikzpicture}
				\vfil
			}
			\caption{Proposition~\ref{lemma:upto} (item 2), Lemma~\ref{mono}, and the restriction to fair computations ensure confluence.}
			\label{fig:conf2}
		\end{minipage}
		\hfil
		
	\end{figure}
	
	
	


Thus, fair computations originating from a configuration are either all finite or all infinite, 
and furthermore  they have the same result.
 
%\subsection{Observational Semantics}
Now, let us denote by $\mathit{Result}(\gamma)$ the result of a configuration 
$\gamma$, which is obtained as the result of any fair computation originating from it.
%
 We are now ready to propose our first semantics.

\begin{definition}[Observational equivalence]\label{def:obequivalence}
Let $A, B \in \mathcal {A}$ be agents. We say that they are observationally equivalent  and we write 
$A \sim_o B$ if 
$\mathit{Result}(\langle A, \sigma \rangle) = \mathit{Result}(\langle B, \sigma \rangle)$
for all $\sigma \in  \C^\otimes$.
%Let $\mathcal{O} : (\mathcal{A} \rrarrow \mathcal{C}^\otimes) \rrarrow \mathcal{C}$ be the function given by 
%$\mathcal{O}({A})(\sigma) = \mathit{Result}(\langle A, \sigma \rangle)$.
%We say that $A$ and $B$ are \emph{observationally equivalent} ($A \sim_o B$) if $\mathcal{O}(A) = \mathcal{O}(B)$.
\end{definition}

It is easy to show $ \sim_o$ to be preserved by all contexts, i.e., it is a \emph{congruence}.
%\footnote{Recall that a context $C[\bullet]$ is a syntactic expression with a single hole $\bullet$ 
%such that replacing $\bullet$ with an agent $A$ in the context produces an agent, denoted by $C[A]$.
% For example if $C[\bullet]$ is the context $\tell(c) \parallel \bullet$  then $C[A] =  \tell(c) \parallel A.$
 %An equivalence $\cong$ between agents is a congruence if $A \cong B$ implies $C[A] \cong C[B]$
  %for every context $C[\bullet].$}


\section{Saturated Bisimulation}\label{sec:saturated}
As proposed in \cite{pippo} for crisp languages, we define a barbed equivalence between two agents~\cite{barbed}.  
%
Intuitively, barbs are basic observations (predicates) on the states of a system, and in our case they correspond 
to the compact constraints in $\mathcal{C}^\otimes$.

\begin{definition} [Barbs]
Let $\langle A, \sigma \rangle$ be a configuration and $c \in \mathcal{C}^\otimes$
and we say that $\langle A, \sigma \rangle$ verifies $c$, or that $\langle A, \sigma \rangle \downarrow_c$ holds, if  $c \leq \sigma$.
\end{definition}

However, since \emph{barbed bisimilarity} is an equivalence already for CCP, along~\cite{pippo}
we propose the use of \emph{saturated bisimilarity}
%~\cite{barbedMontanari} has been proposed 
in order to obtain a congruence:
%
Definition~\ref{def:strongsb} and Definition~\ref{def:weaksb} respectively provide the strong and weak definition of saturated bisimilarity.
%We say that $\gamma = \langle P, \sigma\rangle$ satisfies the barb $c$, written $\gamma \downarrow_c$,
%iff $\gamma \longrightarrow \gamma'$ and $\gamma' \downarrow_c$.
%\marginpar{Are barbs compact?}

\begin{definition}[Saturated bisimilarity]\label{def:strongsb} A saturated bisimulation is a symmetric relation $R$ on configurations such that whenever
%$(\gamma_1,\gamma_2) \in R$ with $\gamma_1 = \langle A, \sigma \rangle$
%and $\gamma_2 = \langle B, \rho \rangle$
$( \langle A, \sigma \rangle,\langle B, \rho \rangle) \in R$
\begin{enumerate}
\item if $\langle A, \sigma \rangle \downarrow_c$ then $\langle B, \rho \rangle \downarrow_c$;
\item if $\langle A, \sigma \rangle \longrightarrow \gamma'_1$ then there is $\gamma'_2$ such that $\langle B, \rho \rangle \longrightarrow \gamma'_2$ and $(\gamma'_1, \gamma'_2) \in R$;
\item $(\langle A,\sigma \otimes d\rangle, \langle B,\rho \otimes d \rangle) \in R$ for  all $d \in \mathcal{C}^\otimes$.
\end{enumerate}
We say that $\gamma_1$ and $\gamma_2$ are  saturated bisimilar ($\gamma_1  \sim_{\mathit{s}} \gamma_2$) if there exists a  saturated  bisimulation $R$ such that $(\gamma_1 , \gamma_2 ) \in R$. We write $A \sim_{\mathit{s}} B$ if $\langle A, \bot\rangle \sim_{\mathit{s}} \langle B, \bot \rangle$.
\end{definition}

We now let $\longrightarrow^*$ denote the reflexive and transitive closure of $\longrightarrow$, restricted to increasing computations.

\begin{definition} [Weak barbs]
Let $\langle A, \sigma \rangle$ be a configuration and $c \in \mathcal{C}^\otimes$.
We say that $\langle A, \sigma \rangle$ weakly verifies $c$, or that $\langle A, \sigma \rangle \Downarrow_c$ holds, 
if  there exists $\gamma' = \langle B, \rho \rangle$ such that 
$\gamma \longrightarrow^* \gamma'$ and $c \leq \exists_{X} \rho$ for $X = fv(\gamma') \setminus fv(\gamma)$.
\end{definition}

\begin{definition}[Weak saturated bisimilarity]\label{def:weaksb} A weak saturated bisimulation is a symmetric relation $R$ on configurations such that whenever
%$(\gamma_1,\gamma_2) \in R$ with $\gamma_1 = \langle A, \sigma \rangle$
%and $\gamma_2 = \langle B, \rho \rangle$
$( \langle A, \sigma \rangle,\langle B, \rho \rangle) \in R$
\begin{enumerate}
\item if $\langle A, \sigma \rangle \downarrow_c$ then $\langle B, \rho \rangle \Downarrow_c$;
\item if $\langle A, \sigma \rangle \longrightarrow \gamma'_1$ then there is $\gamma'_2$ such that $\langle B, \rho \rangle \longrightarrow^* \gamma'_2$ and $(\gamma'_1, \gamma'_2) \in R$;
\item $(\langle A,\sigma \otimes d\rangle, \langle B,\rho \otimes d \rangle) \in R$ for  all $d \in \mathcal{C}^\otimes$.
\end{enumerate}
We say that $\gamma_1$ and $\gamma_2$ are  weakly saturated bisimilar ($\gamma_1  \approx_{\mathit{s}} \gamma_2$) if there exists a  
weak saturated  bisimulation $R$ such that $(\gamma_1 , \gamma_2 ) \in R$. 
We write $A \approx_{\mathit{s}} B$ if $\langle A, \bot\rangle \approx_{\mathit{s}} \langle B, \bot \rangle$.
\end{definition}

The asymmetry is functional to later sections. However, it is clearly equivalent to the standard symmetric version.

\begin{definition}[Weak saturated bisimilarity, 2]\label{def:weaksb2}
Weak saturated bisimilarity coincides with the relation 
obtained from Definition~\ref{def:strongsb} by replacing $\longrightarrow$ with $\longrightarrow^*$ and $\downarrow_c$ with $\Downarrow_c$.
\end{definition}

Since $\sim_{\mathit{s}}$ (and $\approx_{\mathit{s}}$) is a saturated bisimulation, it is clearly upward closed and it is also a congruence: indeed, a context can modify the behaviour of a configuration only by adding constraints to its store.
%\marginpar{to be proved for $\exists_x-$ (not needed for Prop.3)}

\medskip
We now show that $\approx_{\mathit{s}}$, as given in Definition~\ref{def:weaksb}, coincides with the observational equivalence $\sim_o$ (see Definition~\ref{def:obequivalence}). First we recall the notion of and a classic result on \emph{cofinality}: two (possibly infinite) chains $c_0 \leq c_1 \leq \dots$ and  $d_0 \leq d_1 \leq \dots$ are said to be \emph{cofinal} if for all $c_i$ there exists a $d_j$ such that $c_i \leq d_j$ and, vice-versa, for all $d_i$ there exists a $c_j$ such that $d_i \leq c_j$.

\begin{lemma}\label{lem:cofinality} 
Let $c_0 \leq c_1 \leq \dots$ and $d_0 \leq d_1 \leq \dots $ be two chains. \emph{(1)} If they are cofinal, then they have the same limit, i.e., $\bigvee_i c_i = \bigvee_i d_i$. \emph{(2)} If the elements of the chains are $\otimes$-compact and $\bigvee_i c_i = \bigvee_i d_i$, then the two chains are cofinal.\end{lemma}
%\marginpar{questo richiede che tell inserisca solo compatti}
\begin{proof}
%[of Lemma~\ref{lem:cofinality}]
Let us tackle $(2)$, and consider the sequence $e_0 = c_0$ and $e_{i} = c_{i+1} \odiv c_i$.
Each $e_i$ is the difference between two consecutive elements of a chain.
%
Since the CLIM is invertible we have $c_k =  \bigotimes_{i \leq k} e_i$ and thus
$\bigvee_i c_i = \bigotimes_i e_i$. Since each $d_j$ is $\otimes$-compact and
$d_j \leq \bigotimes_i e_i$, there is a $k$ such that $d_j \leq \bigotimes_{i \leq k} e_i$.
The same reasoning is applied to the chain $d_0 \leq d_1 \leq \dots $, thus
the result holds.
\end{proof}


To prove Theorem~\ref{prop:weaksbequivobs}, besides cofinality from Lemma~\ref{lem:cofinality} we need to relate weak barbs and fair computations.

\begin{lemma}\label{lem:barbsfair}
Let $\xi = \gamma_0 \longrightarrow \gamma_1 \longrightarrow \ldots$ be a (possibly infinite) fair computation. If $\gamma_0 \Downarrow_d$ then there exists a store $\sigma_i$ in $\xi$ such that $d \leq \exists_{X_i} \sigma_i$ for $X_i = fv(\gamma_i) \setminus fv(\gamma_0)$.
\end{lemma}

The lemma is a direct consequence of confluence (see Theorem~\ref{prop:confluence}).

\begin{theorem}\label{prop:weaksbequivobs}
Let $A$, $B$ be agents. Then $A \sim_o B$ if and only if $A \approx_{\mathit{s}} B$.
\end{theorem}
%\marginpar{L'unica cosa da provare \`e in realt\`a il Lemma 1}
\begin{proof}%[of Proposition~\ref{prop:weaksbequivobs}]
	The proof proceeds as follows.
	\begin{description}
		\item[From $\approx_{\mathit{s}}$ to $\sim_o$.] Assume  $\langle A, \bot \rangle \approx_{\mathit{s}} \langle B, \bot \rangle$ and take a $\otimes$-compact $c \in \mathcal{C}^\otimes$. Let
		\begin{equation}\label{comp:1}\langle A, c \rangle \longrightarrow \langle A_0, \sigma_0 \rangle \longrightarrow \langle A_1, \sigma_1 \rangle \longrightarrow \dots \longrightarrow \langle A_n, \sigma_n \rangle \dots \longrightarrow \dots
		\end{equation}
		\begin{equation}\label{comp:2}\langle B, c \rangle \longrightarrow \langle B_0, \rho_0 \rangle \longrightarrow \langle B_1, \rho_1 \rangle \longrightarrow \dots \longrightarrow \langle B_n, \rho_n \rangle \dots \longrightarrow \dots
		\end{equation}
		
		be two fair computations. Since $\approx_{\mathit{s}}$ is upward closed, 
		$\langle A, c \rangle \approx_{\mathit{s}} \langle B, c \rangle$ and thus $\langle B, c\rangle \Downarrow_{\sigma_i}$ for all $\sigma_i$. By Lemma~\ref{lem:barbsfair}, it follows that there exists an $\rho_j$ (in the above computation) such that 
		$\exists_{\Gamma_i} \sigma_i \leq \sigma_i \leq \exists_{\Gamma'_j} \rho_j$, and analogously for all $\rho_i$.
		%there exists a $\sigma_j$ such that $\rho_i \leq \sigma_j$. 
		Then $\sigma_0 \leq \sigma_1 \leq \dots$  and $\rho_0 \leq \rho_1 \leq \dots$ are cofinal and by Lemma~\ref{lem:cofinality} it holds that $\bigvee_i \exists_{\Gamma_i} \sigma_i = \bigvee_i \exists_{\Gamma'_i} \rho_i$, which means 
		$\mathit{Result}(\langle A, c \rangle) = \mathit{Result}(\langle B, c \rangle)$.
		%\marginpar{before it was $\bigvee_i \Gamma_i \sigma_i$, no $\exists$}
		
		\item[From $\sim_o$ to $\approx_{\mathit{s}}$.] Assume $A \sim_o B$. First, we show that $\langle A, c\rangle$ and $\langle B, c\rangle$ satisfy the same weak barbs  for all $c \in \mathcal{C}$. Let (\ref{comp:1}) and (\ref{comp:2}) be two fair computations. Since $A \sim_o B$, then $\bigvee_i \exists_{\Gamma_i} \sigma_i = \bigvee_i \exists_{\Gamma'_j} \rho_i$. Since all (the projections of) the intermediate stores of the computations are $\otimes$-compact,
		%(by Lemma~\ref{comp}),
		%\marginpar{da rivedere la sintassi per la tell}
		then by Lemma~\ref{lem:cofinality} for all $\sigma_i$ there exists an $\rho_j$ such that $\exists_{\Gamma_i} \sigma_i \leq \exists_{\Gamma'_j} \rho_j$. 
		Now suppose that $\langle A, c \rangle \Downarrow_d$. By Lemma~\ref{lem:barbsfair}, 
		there exists a $\sigma_i$ 
		%(in the above computation) 
		such that $d \leq \exists_{\Gamma_i} \sigma_i$. Thus 
		%$d \leq \exists_{\Gamma_i} \sigma_i \leq \exists_{\Gamma_i} \rho_j$ that means 
		$\langle B, c\rangle \Downarrow_d$.
		
		It is now easy to prove that
		$R = \{(\gamma_1, \gamma_2) \mid \exists c. \langle A, c \rangle \longrightarrow^* \gamma_1 \& \langle B, c\rangle \longrightarrow^* \gamma_2\}$
		is a weak saturated bisimulation (Definition~\ref{def:weaksb}). Take $(\gamma_1 , \gamma_2 ) \in R$.
		If $\gamma_1 \Downarrow_d$ then $\langle A, c \rangle \Downarrow_d$ and, by the above observation, $\langle B, c\rangle \Downarrow_d$. Since \SCCP is
		confluent, also $\gamma_2 \Downarrow_d$.
		The fact that R is closed under $\longrightarrow^*$ is evident from the definition of $R$. While
		for proving that R is upward-closed take $\gamma_1 = \langle A', \sigma'\rangle$ and $\gamma_2 = \langle B', \rho'\rangle$. By item 4 of Lemma~\ref{mono} (i.e., language monotonicity)
		for all $a \in \mathcal{C}, \langle A, c \otimes a\rangle \longrightarrow^* \langle A', \sigma' \otimes a \rangle$ and $\langle B, c \otimes a\rangle \longrightarrow^* \langle B', \rho'  \otimes a \rangle$. Thus, by definition of $R$, $(\langle A',\sigma' \otimes a \rangle, \langle B',\rho' \otimes a\rangle) \in R$.
	\end{description} 
\end{proof}

\section{Concluding Remarks}\label{sec:conclusion}

\bibliographystyle{splncs03}%splncs
\bibliography{main,softccp}

\end{document}
